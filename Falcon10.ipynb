{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/QJh+cx9kCK/CisosnDgp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrebull/NLP/blob/main/Falcon10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalamos Libreria Oficial de OpenAI"
      ],
      "metadata": {
        "id": "VSFvzrmwsrf9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObewEx7TqEqs",
        "outputId": "1fef2278-baae-47c3-8747-6cdb7252cdeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.91.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "# Instala la librer√≠a oficial de OpenAI para poder conectar y usar sus modelos (como GPT-4) desde Python.\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librerias del Notebook"
      ],
      "metadata": {
        "id": "I_q_xcmmsoRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# Tipolog√≠a 1: Gesti√≥n del Entorno y Sistema Operativo\n",
        "# =================================================================\n",
        "\n",
        "# Se utiliza para interactuar con el sistema operativo, como leer variables de entorno.\n",
        "import os\n",
        "\n",
        "# Espec√≠fico de Google Colab, para acceder a secretos y datos de usuario de forma segura.\n",
        "from google.colab import userdata\n",
        "\n",
        "# =================================================================\n",
        "# Tipolog√≠a 2: Interacci√≥n con APIs Externas\n",
        "# =================================================================\n",
        "\n",
        "# Importa la librer√≠a completa de OpenAI (enfoque antiguo o para funciones espec√≠ficas).\n",
        "import openai\n",
        "\n",
        "# Importa la clase principal para interactuar con la API de OpenAI (enfoque moderno).\n",
        "from openai import OpenAI\n",
        "\n",
        "# =================================================================\n",
        "# Tipolog√≠a 3: Formato y Visualizaci√≥n en Notebooks\n",
        "# =================================================================\n",
        "\n",
        "# Se usa para mostrar texto con formato Markdown en la salida de la celda.\n",
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "5AVME0C5qjNe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargo mi Secret API Key de OpenAI"
      ],
      "metadata": {
        "id": "vrAM5iL1sxjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intenta obtener la clave de API desde los Secretos de Google Colab.\n",
        "api_key = userdata.get(\"JR_OpenAI_Token\")\n",
        "\n",
        "# 1. Verifica si la clave fue encontrada.\n",
        "if api_key:\n",
        "  # 2. Imprime una leyenda bonita si se encontr√≥ la clave.\n",
        "  print(\"‚úÖ ¬°√âxito! Token de API encontrado y cargado correctamente.\")\n",
        "  print(\"---------------------------------------------------------\")\n",
        "  print(\"Inicializando el cliente de OpenAI...\")\n",
        "\n",
        "  # 3. Procede a inicializar el cliente de OpenAI.\n",
        "  client = OpenAI(api_key=api_key)\n",
        "  print(\"ü§ñ Cliente listo. ¬°Ya puedes interactuar con la API!\")\n",
        "\n",
        "else:\n",
        "  # 4. Si no se encuentra, lanza un error claro y √∫til.\n",
        "  raise ValueError(\n",
        "      \"üõë ERROR: Clave de API no encontrada.\\n\"\n",
        "      \"Por favor, aseg√∫rate de haber guardado tu token en los 'Secretos' de Google Colab \"\n",
        "      \"con el nombre exacto: JR_OpenAI_Token\"\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2qWF8LlsUAr",
        "outputId": "e2a545cc-d0f8-4cad-d058-5f812b467af6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ¬°√âxito! Token de API encontrado y cargado correctamente.\n",
            "---------------------------------------------------------\n",
            "Inicializando el cliente de OpenAI...\n",
            "ü§ñ Cliente listo. ¬°Ya puedes interactuar con la API!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definimos los Prompt de Sistema:"
      ],
      "metadata": {
        "id": "OZyRmj_huBbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Asignaci√≥n del prompt\n",
        "# Este prompt solo establece el rol del asistente y el contexto de la empresa.\n",
        "system_prompt = \"\"\"\n",
        "üë§ **Persona y Rol:**\n",
        "\n",
        "Eres un **Asistente Experto** en consultor√≠a de soluciones de Inteligencia Artificial y Procesamiento del Lenguaje Natural (PLN), especializado en el sector bancario.\n",
        "\n",
        "üè¢ **Contexto de la Empresa:**\n",
        "\n",
        "Tu cliente es **Santander US Bank** (filial de Banco Santander S.A., Espa√±a), una instituci√≥n financiera de gran escala en el noreste de EE. UU. con las siguientes m√©tricas:\n",
        "* **Activos:** $147,000,000,000 USD\n",
        "* **Empleados:** 17,200\n",
        "* **Clientes:** 5.2 millones\n",
        "\"\"\"\n",
        "\n",
        "# Imprime la variable para confirmar que se ha guardado correctamente.\n",
        "print(\"‚úÖ Variable 'system_prompt' con Rol y Empresa creada exitosamente.\")\n",
        "print(\"----------------------------------------------------------------\")\n",
        "print(system_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhzS82iOs4TV",
        "outputId": "7d891062-95b8-407a-f996-dad5a9a1bf06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Variable 'system_prompt' con Rol y Empresa creada exitosamente.\n",
            "----------------------------------------------------------------\n",
            "\n",
            "üë§ **Persona y Rol:**\n",
            "\n",
            "Eres un **Asistente Experto** en consultor√≠a de soluciones de Inteligencia Artificial y Procesamiento del Lenguaje Natural (PLN), especializado en el sector bancario.\n",
            "\n",
            "üè¢ **Contexto de la Empresa:**\n",
            "\n",
            "Tu cliente es **Santander US Bank** (filial de Banco Santander S.A., Espa√±a), una instituci√≥n financiera de gran escala en el noreste de EE. UU. con las siguientes m√©tricas:\n",
            "* **Activos:** $147,000,000,000 USD\n",
            "* **Empleados:** 17,200\n",
            "* **Clientes:** 5.2 millones\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "DQ8wVcW-uHXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llamamos a la API de OpenAI:"
      ],
      "metadata": {
        "id": "1nDMbFP6uUbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_openai(system_prompt, user_prompt, model=\"gpt-4o-mini\", temperature=0.7):\n",
        "    \"\"\"\n",
        "    Encapsula una llamada a la API de OpenAI (Chat Completions) para generar una respuesta.\n",
        "\n",
        "    Esta funci√≥n simplifica la interacci√≥n con el modelo, manejando la construcci√≥n\n",
        "    del mensaje y la extracci√≥n del contenido de la respuesta.\n",
        "\n",
        "    Args:\n",
        "        system_prompt (str): El prompt que define el rol y el comportamiento del asistente (ej. \"Eres un experto en Python.\").\n",
        "        user_prompt (str): La pregunta o instrucci√≥n espec√≠fica del usuario.\n",
        "        model (str, optional): El identificador del modelo a utilizar. Por defecto es \"gpt-4o-mini\".\n",
        "        temperature (float, optional): Controla la aleatoriedad de la respuesta. Valores m√°s bajos (~0.2)\n",
        "                                       la hacen m√°s determinista, valores m√°s altos (~1.0) la hacen m√°s creativa.\n",
        "                                       Por defecto es 0.7.\n",
        "\n",
        "    Returns:\n",
        "        str: La respuesta de texto generada por el modelo, limpia de espacios al inicio o al final.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Mensaje de Estado 1: Inicio de la Ejecuci√≥n ---\n",
        "    # Imprimimos un mensaje para saber que la funci√≥n ha comenzado la llamada a la API.\n",
        "    # Usar un f-string nos permite insertar f√°cilmente el nombre del modelo que se est√° usando.\n",
        "    print(f\"üöÄ Iniciando llamada a la API con el modelo: {model}...\")\n",
        "\n",
        "    try:\n",
        "        # --- Llamada a la API de OpenAI ---\n",
        "        # Creamos la solicitud al endpoint de 'chat completions'.\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,                 # Define el modelo de lenguaje a utilizar.\n",
        "            temperature=temperature,     # Controla la \"creatividad\" de la respuesta.\n",
        "            max_tokens=2048,             # L√≠mite m√°ximo de 'tokens' (palabras/fragmentos) en la respuesta. Aumentado para respuestas m√°s largas.\n",
        "            messages=[\n",
        "                # El 'system_prompt' establece el contexto y rol del asistente.\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                # El 'user_prompt' es la pregunta o tarea que le damos al modelo.\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # --- Mensaje de Estado 2: Ejecuci√≥n Exitosa ---\n",
        "        # Si la l√≠nea anterior se completa sin errores, significa que recibimos una respuesta.\n",
        "        print(\"‚úÖ Respuesta recibida exitosamente de OpenAI.\")\n",
        "\n",
        "        # --- Extracci√≥n y Retorno de la Respuesta ---\n",
        "        # El objeto 'response' es complejo. Navegamos hasta el contenido del mensaje.\n",
        "        # .strip() elimina cualquier espacio en blanco o saltos de l√≠nea innecesarios al principio o al final.\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        # --- Manejo de Errores ---\n",
        "        # Si algo sale mal durante la llamada (ej. clave de API incorrecta, problema de red),\n",
        "        # se imprimir√° un error claro.\n",
        "        print(f\"üõë Error al llamar a la API de OpenAI: {e}\")\n",
        "        return None # Devolvemos None para indicar que la funci√≥n fall√≥."
      ],
      "metadata": {
        "id": "-bBKlA1guGYS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Caso 1: User_Prompt Zero-Shot**"
      ],
      "metadata": {
        "id": "-Ynf8NoaunPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt simple y directo (Zero-Shot)\n",
        "# Prompt de usuario, corto y directo para iniciar la interacci√≥n.\n",
        "user_prompt_Zero_Shot = \"Pres√©ntame la idea inicial del proyecto 'SantanderSQL Shield'.\"\n",
        "\n",
        "# (Opcional) Imprime la variable para confirmar que se ha guardado correctamente.\n",
        "print(\"‚úÖ Variable 'user_prompt_zs' (estilo Zero-Shot) creada exitosamente.\")\n",
        "print(\"--------------------------------------------------------------------\")\n",
        "print(user_prompt_Zero_Shot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MhC9MbHuqaJ",
        "outputId": "634d4224-680f-4905-f05c-ce4b371a20f8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Variable 'user_prompt_zs' (estilo Zero-Shot) creada exitosamente.\n",
            "--------------------------------------------------------------------\n",
            "Pres√©ntame la idea inicial del proyecto 'SantanderSQL Shield'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llamamos a la Inteligencia!!!"
      ],
      "metadata": {
        "id": "ke01EEVgw4gY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Informa al usuario que el proceso ha comenzado.\n",
        "print(\"üöÄ Iniciando la consulta Zero-Shot al LLM...\")\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "try:\n",
        "    # Llama a la funci√≥n 'call_openai' con los prompts (que ya existen en el entorno).\n",
        "    # \"Zero-Shot\" significa que se da una instrucci√≥n directa sin ejemplos previos.\n",
        "    respuesta_zero_shot = call_openai(system_prompt, user_prompt_Zero_Shot)\n",
        "\n",
        "    # Presenta un encabezado claro para el resultado.\n",
        "    print(\"\\n‚úÖ Propuesta Generada por el Modelo:\")\n",
        "    print(\"======================================\")\n",
        "\n",
        "    if respuesta_zero_shot:\n",
        "        # Renderiza la respuesta del LLM. Si el modelo us√≥ formato Markdown\n",
        "        # (t√≠tulos, listas, etc.), esto lo mostrar√° de forma legible.\n",
        "        display(Markdown(respuesta_zero_shot))\n",
        "    else:\n",
        "        # Esto se ejecuta si la funci√≥n 'call_openai' no devolvi√≥ nada.\n",
        "        print(\"No se recibi√≥ una respuesta v√°lida del modelo.\")\n",
        "\n",
        "except Exception as e:\n",
        "    # Captura y muestra cualquier error que ocurra durante la llamada a la API.\n",
        "    print(f\"üõë Ocurri√≥ un error durante la ejecuci√≥n: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "ABZ8xnk1w5iS",
        "outputId": "48333d94-f32b-4f06-a01e-d13474bcb7dc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Iniciando la consulta Zero-Shot al LLM...\n",
            "---------------------------------------------\n",
            "üöÄ Iniciando llamada a la API con el modelo: gpt-4o-mini...\n",
            "‚úÖ Respuesta recibida exitosamente de OpenAI.\n",
            "\n",
            "‚úÖ Propuesta Generada por el Modelo:\n",
            "======================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "El proyecto 'SantanderSQL Shield' se propone implementar una soluci√≥n avanzada de seguridad de datos utilizando inteligencia artificial y procesamiento del lenguaje natural (PLN) para proteger la informaci√≥n sensible de los clientes y la instituci√≥n. La idea inicial se basa en los siguientes pilares:\n\n### 1. **Protecci√≥n de Datos Sensibles:**\n   - **Clasificaci√≥n Autom√°tica:** Utilizar algoritmos de PLN para identificar y clasificar autom√°ticamente datos sensibles en tiempo real, como informaci√≥n personal, datos financieros y registros de transacciones.\n   - **Encriptaci√≥n Din√°mica:** Implementar t√©cnicas de encriptaci√≥n que se adapten din√°micamente seg√∫n el nivel de riesgo asociado a los datos identificados, garantizando que solo personal autorizado tenga acceso a la informaci√≥n cr√≠tica.\n\n### 2. **Monitoreo y Detecci√≥n de Amenazas:**\n   - **An√°lisis Predictivo:** Desarrollar modelos de machine learning que analicen patrones de acceso y uso de datos, permitiendo detectar comportamientos an√≥malos que puedan indicar intentos de violaci√≥n de seguridad.\n   - **Alertas en Tiempo Real:** Crear un sistema de alertas que notifique a los equipos de seguridad sobre cualquier actividad sospechosa, facilitando una respuesta r√°pida y efectiva.\n\n### 3. **Cumplimiento Normativo:**\n   - **Auditor√≠a Autom√°tica:** Implementar herramientas que realicen auditor√≠as autom√°ticas de cumplimiento normativo, asegurando que se cumplan las regulaciones de protecci√≥n de datos, como el GDPR y la Ley de Protecci√≥n de la Privacidad del Consumidor de California (CCPA).\n   - **Informes Transparentes:** Generar informes claros y accesibles sobre el estado de la seguridad de los datos, que puedan ser compartidos con las partes interesadas y reguladores.\n\n### 4. **Interfaz de Usuario Intuitiva:**\n   - **Dashboard Interactivo:** Crear un panel de control para los administradores que les permita visualizar en tiempo real el estado de la seguridad de los datos y las alertas de posibles incidentes, facilitando la toma de decisiones informadas.\n\n### 5. **Capacitaci√≥n y Conciencia:**\n   - **Programas de Capacitaci√≥n:** Desarrollar programas de capacitaci√≥n para empleados sobre la importancia de la seguridad de datos y el uso adecuado de la tecnolog√≠a implementada.\n\n### Beneficios Esperados:\n- **Reducci√≥n de Riesgos:** Disminuir la posibilidad de filtraciones de datos y fraudes.\n- **Mejora de la Confianza del Cliente:** Aumentar la confianza de los clientes en la seguridad de sus datos.\n- **Eficiencia Operativa:** Optimizar procesos de cumplimiento y auditor√≠a.\n\n'SantanderSQL Shield' tiene como objetivo establecer un est√°ndar de seguridad de datos en el sector bancario, aline√°ndose con las mejores pr√°cticas y fortaleciendo la reputaci√≥n de Santander US Bank como una instituci√≥n confiable en la protecci√≥n de la informaci√≥n de sus clientes."
          },
          "metadata": {}
        }
      ]
    }
  ]
}