{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state":{},
        "6108ce83ad1b4249821c901665018d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4dfb48f714264cb297fc702ecf4b5ecd",
              "IPY_MODEL_c811e59d30ef4988b1e432e72389fab0",
              "IPY_MODEL_875ba73f53384554a3dfe59d820258c8"
            ],
            "layout": "IPY_MODEL_486048964eda45d781110d36b9fe339b",
            "tabbable": null,
            "tooltip": null
          }
        },
        "4dfb48f714264cb297fc702ecf4b5ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_7cce340849e84e968bcea6646a989f6a",
            "placeholder": "​",
            "style": "IPY_MODEL_c31d15c6a562403696d2c7bdefa7a825",
            "tabbable": null,
            "tooltip": null,
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c811e59d30ef4988b1e432e72389fab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_c84e73d908fd4f7e88cd69d5e353d173",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aed1c3ff8c29481080b5fcf2123e474f",
            "tabbable": null,
            "tooltip": null,
            "value": 3
          }
        },
        "875ba73f53384554a3dfe59d820258c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_1f444f7be910437f9f565194021f58ec",
            "placeholder": "​",
            "style": "IPY_MODEL_372ca5ef171947c9944dd6f7b525d7e9",
            "tabbable": null,
            "tooltip": null,
            "value": " 3/3 [00:05&lt;00:00,  1.70s/it]"
          }
        },
        "486048964eda45d781110d36b9fe339b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cce340849e84e968bcea6646a989f6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c31d15c6a562403696d2c7bdefa7a825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "c84e73d908fd4f7e88cd69d5e353d173": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed1c3ff8c29481080b5fcf2123e474f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f444f7be910437f9f565194021f58ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372ca5ef171947c9944dd6f7b525d7e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## Maestría en Inteligencia Artificial Aplicada\n",
        "#### Tecnológico de Monterrey\n",
        "#### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "### **Adtividad en Equipos: sistema LLM + RAG**"
      ],
      "metadata": {
        "id": "-hVND8xY2OKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Equipo 18**\n",
        "> ### 👨‍💻 **Juan Carlos Pérez Nava**\n",
        "> `A01795941`\n",
        ">\n",
        "> ### 👨‍💻 **Javier Augusto Rebull Saucedo**\n",
        "> `A01795838`\n",
        ">\n",
        "> ### 👩‍💻 **Sihiní Trinidad Sánchez**\n",
        "> `A00889358`\n",
        ">\n",
        "> ### 👩‍💻 **Iris Monserrat Urbina Casas**\n",
        "> `A01795999`\n",
        "\n"
      ],
      "metadata": {
        "id": "aimHVFOv23lm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ##### **El formato de este cuaderno de Jupyter es libre, pero debe incuir al menos las siguientes secciones:**\n",
        "\n",
        "  * ##### **Introducción de la problemática a resolver.**\n",
        "  * ##### **Sistema RAG + LLM**\n",
        "  * ##### **El chatbot, incluyendo ejemplos de prueba.**\n",
        "  * ##### **Conclusiones**\n",
        "\n",
        "* ##### **Pueden importar los paquetes o librerías que requieran.**\n",
        "\n",
        "* ##### **Pueden incluir las celdas y líneas de código que deseen.**"
      ],
      "metadata": {
        "id": "7jimvsiVgjMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción de la problemática a resolver\n",
        "\n",
        "En las áreas operativas del Instituto Mexicano del Seguro Social existen diversas oportunidades de mejora en la atención al público, la resolución de problemas y el entendimiento del negocio y sus procedimientos. Por ello, se ha diseñado la integración de normas, reglas y procedimientos específicos para la atención en cada una de las ventanillas de la Dirección de Incorporación y Recaudación, encargada de recibir las solicitudes de registro de los patrones y sus trabajadores, así como de gestionar la recaudación de las aportaciones patronales. Sin embargo, identificamos como problemática la dificultad que enfrentan muchas personas para entender los requisitos, documentos, horarios, modalidades, etc. Aunque la información oficial está disponible en línea, suele estar dispersa, fragmentada y en un lenguaje técnico y difícil de interpretar.\n",
        "\n",
        "\n",
        "Con el objetivo de mejorar el acceso a esta información y optimizar la experiencia del usuario, desarrollamos un chatbot basado en un modelo de lenguaje de gran tamaño (LLM), complementado con un sistema de Recuperación Aumentada Generativa (RAG). Esta arquitectura permite generar respuestas precisas y contextualizadas a partir de documentos reales del IMSS, mejorando así la fidelidad factual del modelo (Lewis et al., 2020).\n",
        "\n",
        "\n",
        "El corpus utilizado fue construido a partir de documentos recuperados de la sección oficial de “Trámites y servicios” del IMSS (2025), los cuales explican procesos clave como el alta patronal y la gestión de incidencias en el sistema. Estos documentos contienen instrucciones detalladas que garantizan que cada situación pueda ser atendida de forma eficiente y conforme a la normatividad vigente.\n",
        "\n",
        "\n",
        "Este chatbot busca ser una herramienta de apoyo para ciudadanos, empresarios y trabajadores, facilitando la comprensión de los procedimientos institucionales y apoyando la resolución autónoma de dudas durante la gestión de sus trámites.\n",
        "\n",
        "\n",
        "**Referencias:**\n",
        "\n",
        "* Lewis, M., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. arXiv preprint arXiv:2005.11401 Recuperado el 20 de junio de 2025. https://arxiv.org/abs/2005.11401\n",
        "\n",
        "* IMSS. (2025). Trámites y servicios. Recuperado el 22 de junio de 2025. https://www.imss.gob.mx/tramites\n"
      ],
      "metadata": {
        "id": "WVw8V0cBrFm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sistema RAG + LLM"
      ],
      "metadata": {
        "id": "4EY70nzmrHl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el contexto del desarrollo de nuestro chatbot para trámites del IMSS, implementamos arquitectura de Generación Aumentada por Recuperación (RAG), un enfoque que combina la capacidad generativa de los modelos de lenguaje (LLM) con mecanismos de recuperación de documentos relevantes. Como lo aprendimos durante el curso y la actividad de IBM de Generative AI in Action, esta arquitectura ha demostrado mejorar la precisión, actualidad y fiabilidad de las respuestas generadas (AWS, 2025; IBM, 2025; Lewis et al., 2020).\n",
        "\n",
        "\n",
        "**¿Cómo funciona RAG?**\n",
        "\n",
        "AWS (2025) RAG explica que RAG incorpora un mecanismo de recuperación de información antes de generar una respuesta, lo que permite que el modelo se base en datos actualizados y verificables. El LLM recibe una pregunta del usuario y antes de responder, consulta un repositorio de información actualizado, que puede ser una fuente abierta como la web o una base de datos específica. Después, genera la respuesta basada en los datos recuperados, proporcionando evidencia o fuentes verificables para respaldar su contenido.\n",
        "\n",
        "\n",
        "En nuestro chatbot, el sistema RAG opera en tres fases principales:\n",
        "1. Recepción de la consulta del usuario.\n",
        "2. Recuperación semántica de fragmentos de documentos relevantes usando un índice vectorial construido previamente a partir de fuentes oficiales del IMSS.\n",
        "3. Generación de una respuesta contextualizada, donde el modelo de lenguaje integra la información recuperada para formular una respuesta coherente y fundamentada.\n",
        "\n",
        "\n",
        "Este enfoque permite que el chatbot no dependa exclusivamente de conocimiento preentrenado (potencialmente desactualizado), sino que acceda en tiempo real a documentación actualizada y específica del dominio institucional.\n",
        "\n",
        "\n",
        "**Beneficios (IBM, 2025; Lewis, 2020)**\n",
        "\n",
        "\n",
        "* Mayor precisión y actualidad: Evita respuestas\n",
        "erróneas o desactualizadas al basarse en información reciente.\n",
        "* Reducción de \"alucinaciones\": Minimiza la generación de datos inventados por el modelo.\n",
        "* Mayor confiabilidad: Permite que el modelo reconozca cuando no tiene una respuesta confiable, evitando la generación de información incorrecta\n",
        "\n",
        "\n",
        "**Advertencias**\n",
        "\n",
        "Para que RAG funcione de manera óptima, el sistema de recuperación debe ser eficiente y proporcionar datos relevantes y de alta calidad. Si la información recuperada es insuficiente o poco precisa, el modelo podría no responder correctamente, incluso si la respuesta existe en alguna fuente confiable. (Como todo en IA, \"garbage in, garbage out!\"  Por ello, seleccionamos documentos directamente de fuentes oficiales del IMSS como corpus para garantizar su relevancia y validez.\n",
        "\n",
        "\n",
        "**Especificaciones del RAG implementado**\n",
        "\n",
        "Para nuestra solución, elegimos los siguientes componentes técnicos:\n",
        "\n",
        "* Modelo de lenguaje grande (LLM): \"mistralai/Mistral-7B-Instruct-v0.2.\"\n",
        "* Embeddings: HuggingFaceEmbeddings con modelo multilingüe (sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2), ideal para recuperar contenido en español.\n",
        "* División del corpus: Mediante RecursiveCharacterTextSplitter, con chunk_size=1500 y chunk_overlap=500, para conservar el contexto entre fragmentos.\n",
        "* Almacenamiento vectorial: FAISS como índice de recuperación, por su eficiencia en búsqueda semántica.\n",
        "* Pipeline de recuperación: LangChain, configurado para buscar los topk=50fragmentos más relevantes por consulta.\n",
        "\n",
        "\n",
        "**Referencias**\n",
        "\n",
        "* AWS. (2025). What is retrieval augmented generation? Recuperado el 15 de junio de 2025 de https://aws.amazon.com/es/what-is/retrieval-augmented-generation/\n",
        "\n",
        "* IBM. (2025). Generative AI in Action. IBM SkillsLab. Recuperado el 15 de junio de 2025 de https://www.ibm.com/academic/topic/artificial-intelligence?ach_id=9217fca4-c36f-4e1d-ab87-da43469344a5\n",
        "\n",
        "* ewis, M., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. arXiv preprint arXiv:2005.11401. https://arxiv.org/abs/2005.11401"
      ],
      "metadata": {
        "id": "KFFxZqDxtPWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalación de librerías"
      ],
      "metadata": {
        "id": "NDXUlSJHBx3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Instalamos las librerías necesarias para construir un chatbot RAG con LLM, recuperación semántica, procesamiento de PDFs e interfaz con Gradio.\n",
        "!pip install torch transformers sentence-transformers faiss-cpu langchain pypdf accelerate bitsandbytes langchain-community gradio PyMuPDF"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JgI1rNFOZw_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f3fcb68-240a-4c04-c89d-db338ac6c4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in ./miniconda3/envs/torch/lib/python3.12/site-packages (2.8.0.dev20250610+cu128)\r\n",
            "Requirement already satisfied: transformers in ./miniconda3/envs/torch/lib/python3.12/site-packages (4.52.4)\r\n",
            "Requirement already satisfied: sentence-transformers in ./miniconda3/envs/torch/lib/python3.12/site-packages (4.1.0)\r\n",
            "Requirement already satisfied: faiss-cpu in ./miniconda3/envs/torch/lib/python3.12/site-packages (1.11.0)\r\n",
            "Requirement already satisfied: langchain in ./miniconda3/envs/torch/lib/python3.12/site-packages (0.3.25)\r\n",
            "Requirement already satisfied: pypdf in ./miniconda3/envs/torch/lib/python3.12/site-packages (5.6.0)\r\n",
            "Requirement already satisfied: accelerate in ./miniconda3/envs/torch/lib/python3.12/site-packages (1.7.0)\r\n",
            "Requirement already satisfied: bitsandbytes in ./miniconda3/envs/torch/lib/python3.12/site-packages (0.46.0)\r\n",
            "Requirement already satisfied: langchain-community in ./miniconda3/envs/torch/lib/python3.12/site-packages (0.3.25)\r\n",
            "Requirement already satisfied: gradio in ./miniconda3/envs/torch/lib/python3.12/site-packages (5.33.1)\r\n",
            "Requirement already satisfied: PyMuPDF in ./miniconda3/envs/torch/lib/python3.12/site-packages (1.26.1)\r\n",
            "Requirement already satisfied: filelock in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (3.18.0)\r\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (4.14.0)\r\n",
            "Requirement already satisfied: setuptools in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (78.1.1)\r\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (1.14.0)\r\n",
            "Requirement already satisfied: networkx in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (3.5)\r\n",
            "Requirement already satisfied: jinja2 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (3.1.6)\r\n",
            "Requirement already satisfied: fsspec in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (2025.5.1)\r\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (12.8.93)\r\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (12.8.90)\r\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (12.8.90)\r\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (9.8.0.87)\r\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (12.8.4.1)\r\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (11.3.3.83)\r\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (10.3.9.90)\r\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (11.7.3.90)\r\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (12.5.8.93)\r\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (0.7.1)\r\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.5 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (2.26.5)\r\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.2.5 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (3.2.5)\r\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (12.8.90)\r\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (12.8.93)\r\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (1.13.1.3)\r\n",
            "Requirement already satisfied: pytorch-triton==3.3.1+gitc8757738 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (3.3.1+gitc8757738)\r\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (0.32.5)\r\n",
            "Requirement already satisfied: numpy>=1.17 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (2.3.0)\r\n",
            "Requirement already satisfied: packaging>=20.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (24.2)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (6.0.2)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (2024.11.6)\r\n",
            "Requirement already satisfied: requests in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (2.32.4)\r\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (0.21.1)\r\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (0.5.3)\r\n",
            "Requirement already satisfied: tqdm>=4.27 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: scikit-learn in ./miniconda3/envs/torch/lib/python3.12/site-packages (from sentence-transformers) (1.7.0)\n",
            "Requirement already satisfied: scipy in ./miniconda3/envs/torch/lib/python3.12/site-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in ./miniconda3/envs/torch/lib/python3.12/site-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio in ./miniconda3/envs/torch/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: certifi in ./miniconda3/envs/torch/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in ./miniconda3/envs/torch/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in ./miniconda3/envs/torch/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: psutil in ./miniconda3/envs/torch/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain-community) (3.12.12)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.3 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (1.10.3)\n",
            "Requirement already satisfied: groovy~=0.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (2.3.0)\n",
            "Requirement already satisfied: pydub in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.11.13)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio-client==1.10.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: click>=8.0.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
            "Requirement already satisfied: six>=1.5 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Librerías para Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "Estas bibliotecas permiten cargar documentos, dividirlos en fragmentos y representarlos como vectores para una búsqueda eficiente.\n",
        "- `TextLoader`: Carga documentos de texto.\n",
        "- `CharacterTextSplitter` y `RecursiveCharacterTextSplitter`: Dividen los documentos en fragmentos más pequeños para su procesamiento.\n",
        "- `HuggingFaceEmbeddings`: Convierte texto en representaciones vectoriales mediante modelos de embeddings.\n",
        "- `FAISS`: Motor de búsqueda basado en índices de vectores, diseñado para recuperar información de manera rápida y eficiente.\n",
        "\n",
        "🔹 Librerías para el Modelo de Lenguaje Grande (LLM)\n",
        "\n",
        "Estas herramientas permiten generar respuestas utilizando modelos de lenguaje basados en Transformers.\n",
        "- `AutoModelForCausalLM` y `AutoTokenizer`: Cargan y configuran el modelo de generación de texto.\n",
        "- `BitsAndBytesConfig`: Optimiza la ejecución del modelo para reducir el consumo de memoria.\n",
        "- `pipeline`: Establece un flujo de procesamiento que facilita la generación de texto.\n",
        "- `torch`: Biblioteca de aprendizaje profundo utilizada para el manejo y ejecución de modelos de lenguaje.\n",
        "\n"
      ],
      "metadata": {
        "id": "T_YbgETuz5d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importaciones para el sistema RAG (Retrieval-Augmented Generation)\n",
        "from langchain.document_loaders import TextLoader  # Carga documentos de texto\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter  # Divide el texto en fragmentos para facilitar el procesamiento\n",
        "from langchain_huggingface import HuggingFaceEmbeddings  # Para generar vectores de texto con modelos de Hugging Face\n",
        "from langchain_community.vectorstores import FAISS  # FAISS para almacenamiento y búsqueda eficiente de vectores\n",
        "\n",
        "# Importaciones para el modelo LLM (Generación de respuestas)\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline  # Para cargar y utilizar modelos de lenguaje\n",
        "import torch  # Biblioteca para operaciones con tensores y aceleración en GPU\n",
        "\n",
        "# Autenticación con Hugging Face Hub para acceder a modelos\n",
        "from huggingface_hub import login\n",
        "login(token=\"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\")\n",
        "\n",
        "# Elementos para construir la cadena RAG con Langchain\n",
        "from langchain.prompts import PromptTemplate  # Plantilla de prompt para el modelo de lenguaje\n",
        "from langchain.schema.runnable import RunnablePassthrough  # Permite que una función pase datos sin modificarlos\n",
        "from langchain_community.llms import HuggingFacePipeline  # Integra modelos de Hugging Face como LLM en Langchain\n",
        "from langchain.schema import StrOutputParser  # Analiza salidas del modelo como texto plano\n",
        "\n",
        "# Librería Gradio para construir interfaces web interactivas\n",
        "import gradio as gr\n",
        "\n",
        "# Módulos auxiliares para rutas, expresiones regulares, descarga de archivos y lectura de PDFs\n",
        "import os  # Manejo de rutas de archivos y directorios\n",
        "import re  # Manejo de expresiones regulares\n",
        "import gdown  # Descarga de archivos desde Google Drive\n",
        "import fitz  # Lectura y manipulación de documentos PDF (PyMuPDF)\n",
        "\n",
        "# Definición del modelo de embeddings\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Definición del modelo de lenguaje grande (LLM)\n",
        "LLM_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# Rutas y configuraciones para los documentos\n",
        "directorio_documentos = 'datos_RAG_LLM'  # Carpeta donde se almacenarán los documentos para el sistema RAG\n",
        "id_repositorio = '18z_pJe7EcZO6oqCSebqS_Qe-E1kL4RUH'  # ID de Google Drive para descargar los documentos\n",
        "\n",
        "# Mensajes de confirmación para mostrar que las librerías y rutas han sido configuradas correctamente\n",
        "print(\"\\033[32mLibrerías importadas:\\033[0m\")\n",
        "print(f\"Modelo de Embeddings: \\033[36m{EMBEDDING_MODEL_NAME}\\033[0m\")\n",
        "print(f\"Modelo LLM: \\033[36m{LLM_MODEL_NAME}\\033[0m\")\n",
        "\n",
        "print(\"\\033[32mRutas configuradas:\\033[0m\")\n",
        "print(f\"Carpeta de documentos: \\033[36m{directorio_documentos}\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe5MFyfVR2uW",
        "outputId": "b95eddce-bf29-4fa3-a78b-1663ed201025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mLibrerías importadas:\u001b[0m\n",
            "Modelo de Embeddings: \u001b[36msentence-transformers/all-MiniLM-L6-v2\u001b[0m\n",
            "Modelo LLM: \u001b[36mmistralai/Mistral-7B-Instruct-v0.2\u001b[0m\n",
            "\u001b[32mRutas configuradas:\u001b[0m\n",
            "Carpeta de documentos: \u001b[36mdatos_RAG_LLM\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación del Corpus Documental"
      ],
      "metadata": {
        "id": "kZVb0NEBMaJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recolección de documentos**\n",
        "\n",
        "Para garantizar que el chatbot generara respuestas precisas y contextualizadas, construimos un corpus documental específico a partir de fuentes oficiales del Instituto Mexicano del Seguro Social (IMSS), particularmente de su sección de “Trámites y servicios” (IMSS, 2025). Este corpus contiene información detallada sobre procesos como registro patronal, afiliación voluntaria, incapacidades y corrección de datos, entre otros.\n",
        "\n",
        "Los documentos fueron seleccionados manualmente desde el sitio oficial del IMSS, asegurando su relevancia y actualidad. Se priorizaron guías, instructivos y fichas de trámites que explican paso a paso cada procedimiento.\n",
        "\n",
        "**Extracción y limpieza**\n",
        "\n",
        "Los documentos se encontraban en formato PDF, por lo que utilizamos la librería PyMuPDF (fitz) para extraer su contenido textual. Este método permite preservar la estructura del texto con mayor fidelidad que otras alternativas como pypdf. Durante esta fase se eliminaron encabezados, pies de página y páginas en blanco y se normalizó la codificación (UTF-8) y se corrigieron errores de segmentación y caracteres extraños.\n",
        "\n",
        "**División del contenido en fragmentos**\n",
        "\n",
        "Una parte esencial del sistema RAG es la división del texto en fragmentos o chunks antes de generar sus vectores. Esta segmentación permite construir un índice de recuperación semántica más efectivo y relevante. En nuestro caso, usamos RecursiveCharacterTextSplitter de LangChain con los parámetros: chunk_size = 1500 caracteres; chunk_overlap = 500 caracteres\n",
        "\n",
        "Esta configuración permite que los fragmentos tengan suficiente contexto para ser útiles durante la recuperación, mientras el solapamiento (overlap) evita la pérdida de información crítica al final de cada fragmento.\n",
        "\n",
        "La literatura técnica en modelos RAG enfatiza que una división adecuada del texto mejora la precisión del sistema de recuperación, ya que los modelos de embeddings generan mejores vectores semánticos cuando operan sobre unidades coherentes de contenido (Izacard & Grave, 2020). LangChain sugiere que este enfoque también reduce los errores comunes del modelo, como respuestas alucinadas o sin fundamento, al asegurar que la información relevante esté presente durante la fase de generación (LangChain, 2025).\n",
        "\n",
        "**Referencias**\n",
        "* Izacard, G., & Grave, E. (2020). Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. arXiv. https://arxiv.org/abs/2007.01282\n",
        "\n",
        "LangChain. (2025). Text Splitters. Recuperado el 22 de junio de 2025 de https://python.langchain.com/docs/concepts/text_splitters/\n",
        "\n",
        "Hugging Face. (2024). Dealing with Chunked Input Text and Summaries for Fine Tuning Summarization model. Recuperado el 22 de junio de 2025 de https://discuss.huggingface.co/t/dealing-with-chunked-input-text-and-summaries-for-fine-tuning-summarization-model/77186\n",
        "\n"
      ],
      "metadata": {
        "id": "WjsZNzaLMeeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Descargar los documentos para el modelo\n",
        "print(f\"📂 Ruta de los documentos → \\033[36m'{directorio_documentos}\\033[0m'\\n\")\n",
        "\n",
        "# Crear el directorio si no existe\n",
        "if not os.path.exists(directorio_documentos):\n",
        "    os.makedirs(directorio_documentos)\n",
        "\n",
        "gdown.download_folder(id=id_repositorio, output=directorio_documentos, quiet=False, use_cookies=False)\n",
        "\n",
        "archivos_cargados = os.listdir(directorio_documentos)\n",
        "for archivo in archivos_cargados:\n",
        "    print(f\"📂 Documento cargado: \\033[36m{archivo}\\033[0m\")\n",
        "\n",
        "print(f\"Total de documentos descargados: \\033[32m{len(archivos_cargados)}\\033[0m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4ptoOud26iNb",
        "outputId": "71019730-7e06-47c0-e4a5-de630166914f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Ruta de los documentos → \u001b[36m'datos_RAG_LLM\u001b[0m'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1q1I1zt_ygtTYjYjKewFq4zf4g0NkxsVA 9000-002-002.pdf\n",
            "Processing file 1a-f4o7ZyuYHeEZBsQ2GVfsY5p4D1Lhwm 9000-002-003.pdf\n",
            "Processing file 1in4iGXlLxQQXeu-3SGeSf7q6Jg80en8t 9210-003-101.pdf\n",
            "Processing file 1KvTvJqYUNMALFxghrxu_ubnxYBoQxeig 9210-003-105.pdf\n",
            "Processing file 1Dx79o2digN-QuVgplrlUrcN_4AUkSMPA 9210-003-112.pdf\n",
            "Processing file 17skF_0icdMo6t2c2rkmfpetvBh482JCy 9210-003-114.pdf\n",
            "Processing file 1fsAN6bJ_giDsiawRj4aTErkke940AhBD 9210-003-115.pdf\n",
            "Processing file 10rXmodDefLNjsAQAom7M-nJQgM9_3mzF 9210-003-124.pdf\n",
            "Processing file 1eF-y19UXMTZ6eO-JauGVG9JxngkAl03j 9210-003-125.pdf\n",
            "Processing file 1_QRQ5wEc2ehj5AvVca3E_j4UWXD7H68r 9210-003-126.pdf\n",
            "Processing file 1QynDJpMW3o04Rq7MjzHUerkzoWHfF-GI 9210-003-200.pdf\n",
            "Processing file 1QAmniZXbTS7xUueXpR_90fEe4wJMJK3_ 9210-003-201.pdf\n",
            "Processing file 12Tf_lk8cztw_qzTAAwElinNTKkfr-QTx 9210-003-202.pdf\n",
            "Processing file 19E3EQ0QkW18nlxS0XHqiPKfjqBoOXgno 9210-003-209.pdf\n",
            "Processing file 1xE1hqX5HPfnXgtz-BszJt64bu7ghgo-Y 9210-003-210.pdf\n",
            "Processing file 1zS8rkqIxFwEfDT9HeD1NFeAbxzOYKAMX 9210-003-211.pdf\n",
            "Processing file 1suynU_UdXSs2-T3dxbdYElL6cGkfQmfg 9210-003-224.pdf\n",
            "Processing file 1I87WNVstNjBrljDz9El6oZ6TH55uDrC5 9210-003-601.pdf\n",
            "Processing file 114KdjaOU3Qbsy63lTIzgl9hhEQt_YMHx 9210-003-604.pdf\n",
            "Processing file 1zvXiTlJEW-cANkHPDKvWH9Eh6QVMwdeY 9210-003-606.pdf\n",
            "Processing file 1D0rHTrpG0TErw80eaK10r47y093SDAUL 9210-003-607.pdf\n",
            "Processing file 1RFQ09MgbsFOGVat599QhjYnxY9ANEacL 9210-003-611.pdf\n",
            "Processing file 1TUiFTusbbuJbiF5T92XIpIjq5s6AqSNC 9210-003-612.pdf\n",
            "Processing file 1vDeU8zXZcGzSxbQGHTSptGkTvIDlyRVC 9210-005-101.pdf\n",
            "Processing file 1E-VewfltA-TcqjlUcYZfvjTZiwKex6uZ 9210-005-102.pdf\n",
            "Processing file 1F1lH7FkCAMBJxhxeN3Qo9m52dMvV4f5u 9210-005-132.pdf\n",
            "Processing file 13c405LnqAm9bz4mUQ9fd7V3dPFaA30cN 9210-005-202.pdf\n",
            "Processing file 1T_Y8N3cyBXF1892En1_NDYfXSRBT6IFf 9210-005-205.pdf\n",
            "Processing file 1IIt90nmrhwMTau8KimX9CZWqt0hxtS2k 9210-005-206.pdf\n",
            "Processing file 1cjJIwcH26qRgpgVmfVIWldOxRM7c53uB 9210-005-208.pdf\n",
            "Processing file 1TzZqjRgnWQbNM_bETJ2Bcsl5An5zCs9Z 9210-005-209.pdf\n",
            "Processing file 1asArwphfXAWuRtbG0KOnVS6m7rUIAzUX 9210-005-210.pdf\n",
            "Processing file 1pTcFnyk_fJfoMX8ul8v764mDDzXgnhQ2 9210-005-211.pdf\n",
            "Processing file 1Z60O_CdtkOroRJGIQ_EQ_-LcpM2N1uw_ 9210-005-213.pdf\n",
            "Processing file 1_qcvNcFygur59YwMQx4nZ7tU5Nfet9zK 9210-005-214.pdf\n",
            "Processing file 1Jx7kFMMDfKXW1AULTLu_6hfTn8Vv1AUg 9210-008-101.pdf\n",
            "Processing file 1iibLQKaIyBoSgHDN3NBFK6V6OIf65icL 9210-008-203.pdf\n",
            "Processing file 1dGfuMVjgwFrdvPrp5Xe2HT4m3idziOnt 9210-008-204.pdf\n",
            "Processing file 1Y57i1EeLSLEXD_eQGNn_uvjrrvonh2Pr 9210-018-200.pdf\n",
            "Processing file 1Y5gcWNAb8j-J7AGY3NNVVbaK7PfD906q Filtros.txt\n",
            "Processing file 1UxaGkvtTj9pomdp7KR5Fs7lbUAxJT31e LSS.pdf\n",
            "Processing file 1R7LDR_9uN39o6FDUACiWSbmq9nEzXk5q Procedimiento Alta Patronal.txt\n",
            "Processing file 1dnoH0CJGEXUV8WULeXuG5HLgMOJTbJdT Reg_LSS_MACERF.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileURLRetrievalError",
          "evalue": "Failed to retrieve file url:\n\n\tCannot retrieve the public link of the file. You may need to change\n\tthe permission to 'Anyone with the link', or have had many accesses.\n\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1q1I1zt_ygtTYjYjKewFq4zf4g0NkxsVA\n\nbut Gdown can't. Please check connections and permissions.",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileURLRetrievalError\u001b[39m                     Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.12/site-packages/gdown/download.py:267\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     url = \u001b[43mget_url_from_gdrive_confirmation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.12/site-packages/gdown/download.py:55\u001b[39m, in \u001b[36mget_url_from_gdrive_confirmation\u001b[39m\u001b[34m(contents)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(\n\u001b[32m     56\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot retrieve the public link of the file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou may need to change the permission to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     58\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAnyone with the link\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, or have had many accesses. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     59\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     60\u001b[39m     )\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m url\n",
            "\u001b[31mFileURLRetrievalError\u001b[39m: Cannot retrieve the public link of the file. You may need to change the permission to 'Anyone with the link', or have had many accesses. Check FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mFileURLRetrievalError\u001b[39m                     Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(directorio_documentos):\n\u001b[32m      6\u001b[39m     os.makedirs(directorio_documentos)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mgdown\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mid_repositorio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdirectorio_documentos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cookies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m archivos_cargados = os.listdir(directorio_documentos)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m archivo \u001b[38;5;129;01min\u001b[39;00m archivos_cargados:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.12/site-packages/gdown/download_folder.py:325\u001b[39m, in \u001b[36mdownload_folder\u001b[39m\u001b[34m(url, id, output, quiet, proxy, speed, use_cookies, remaining_ok, verify, user_agent, skip_download, resume)\u001b[39m\n\u001b[32m    322\u001b[39m     files.append(local_path)\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m local_path = \u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://drive.google.com/uc?id=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cookies\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.12/site-packages/gdown/download.py:278\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[39m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    269\u001b[39m         message = (\n\u001b[32m    270\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFailed to retrieve file url:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    271\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou may still be able to access the file from the browser:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    276\u001b[39m             url_origin,\n\u001b[32m    277\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(message)\n\u001b[32m    280\u001b[39m filename_from_url = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    281\u001b[39m last_modified_time = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[31mFileURLRetrievalError\u001b[39m: Failed to retrieve file url:\n\n\tCannot retrieve the public link of the file. You may need to change\n\tthe permission to 'Anyone with the link', or have had many accesses.\n\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1q1I1zt_ygtTYjYjKewFq4zf4g0NkxsVA\n\nbut Gdown can't. Please check connections and permissions."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertir los archivos PDF descargados en un documento de texto"
      ],
      "metadata": {
        "id": "7XW2g8FsCakJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener archivos PDF en el directorio\n",
        "archivos_pdf = [f for f in os.listdir(directorio_documentos) if f.lower().endswith(\".pdf\")]\n",
        "\n",
        "for archivo in archivos_pdf:\n",
        "    ruta_archivo = os.path.join(directorio_documentos, archivo)\n",
        "    texto_extraido = \"\"\n",
        "    try:\n",
        "        with fitz.open(ruta_archivo) as documento:\n",
        "            for pagina in documento:\n",
        "                texto_extraido += pagina.get_text()\n",
        "\n",
        "                texto_extraido = texto_extraido.replace('\\xad', '')\n",
        "                texto_dividido = texto_extraido.splitlines()\n",
        "                texto_limpio = [re.sub(r'\\s+', ' ', re.sub(r'\\\\[a-zA-Z0-9]{1,6}', '', linea)).strip() for linea in texto_dividido]\n",
        "                lineas_utiles = [linea for linea in texto_limpio if linea.strip() and len(linea.strip().split()) > 1 and not linea.strip().isdigit()]\n",
        "\n",
        "                nombre_documento = os.path.splitext(archivo)[0] + \".txt\"\n",
        "                ruta_archivo_txt = os.path.join(directorio_documentos, nombre_documento)\n",
        "\n",
        "                with open(ruta_archivo_txt, \"w\", encoding=\"utf-8\") as archivo_txt:\n",
        "                  archivo_txt.write(\"\\n\".join(lineas_utiles))\n",
        "\n",
        "            print(f\"📄 Documento .txt creado: \\033[36m{ruta_archivo_txt}\\033[0m\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo procesar {archivo}: {e}\")\n",
        "\n",
        "print(f\"Total de documentos generados: \\033[32m{len(archivos_pdf)}\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLcARwERAMmn",
        "outputId": "fb94edaf-f3c1-406c-e0a1-98466efee31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-202.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-126.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-018-200.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-205.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-008-204.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-124.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-201.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-125.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-115.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-102.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/Reg_LSS_MACERF.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-008-203.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9000-002-002.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-209.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-213.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-607.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-210.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-101.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-114.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-132.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-200.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-211.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-208.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-206.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-612.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-606.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-101.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-604.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9000-002-003.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/LSS.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-008-101.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-611.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-112.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-202.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-211.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-214.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-210.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-224.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-209.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-601.txt\u001b[0m\n",
            "📄 Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-105.txt\u001b[0m\n",
            "Total de documentos generados: \u001b[32m41\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creación de la base de datos FAISS"
      ],
      "metadata": {
        "id": "5yPOJfukCMoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para implementar el componente de recuperación del sistema RAG, utilizamos FAISS (Facebook AI Similarity Search), una biblioteca diseñada por Meta para la búsqueda de similitudes entre vectores densos. Este sistema es utilizado en aplicaciones que requieren búsquedas rápidas y precisas en grandes volúmenes de datos, como recomendadores, motores de búsqueda semántica, y sistemas de respuesta a preguntas (Meta AI, 2017; Douze et al., 2024).\n",
        "\n",
        "En nuestro chatbot FAISS almacena los embeddings generados a partir de fragmentos del corpus documental, permitiendo recuperar los pasajes más relevantes en tiempo real. A continuación, describimos el flujo técnico utilizado:\n",
        "\n",
        "1️⃣ Carga del Documento\n",
        "- `TextLoader` abre el archivo de texto en la ubicación DATA_PATH.\n",
        "- `loader.load()` lee el contenido y lo almacena en documents.\n",
        "- Se imprime la cantidad de documentos cargados para verificación.\n",
        "\n",
        "2️⃣ División del Texto en Fragmentos (chunks)\n",
        "- `CharacterTextSplitter` divide los documentos en partes 1500 caracteres con un solapamiento de 500 caracteres.\n",
        "- Esto ayuda a mejorar las búsquedas sin perder el contexto entre fragmentos (LangChain, 2025)\n",
        "\n",
        "3️⃣ Generación de Embeddings Semánticos\n",
        "- `HuggingFaceEmbeddings` carga el modelo de embeddings, que convierte texto en vectores numéricos.\n",
        "- Estos vectores representan el significado de las palabras y permiten búsquedas semánticas.\n",
        "\n",
        "4️⃣ Construcción del índice vectorial con FAISS\n",
        "- `FAISS.from_documents(docs, embeddings)` crea una base de datos donde cada fragmento de texto se convierte en un vector mediante los embeddings.\n",
        "- `FAISS` permite hacer búsquedas rápidas y eficientes dentro de este conjunto de datos vectoriales.\n",
        "\n",
        "\n",
        "**Referencias**\n",
        "\n",
        "* Meta AI. (2017, marzo 29). Faiss: A library for efficient similarity search. Engineering at Meta. Recuperado el 23 de junio de 2025 de https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/\n",
        "* Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré, P.-E., et al. (2024). The Faiss library. arXiv. Recuperado el 23 de junio de https://arxiv.org/pdf/2401.08281\n",
        "* LangChain. (2025). Text Splitters. Recuperado el 22 de junio de 2025 de https://python.langchain.com/docs/concepts/text_splitters/"
      ],
      "metadata": {
        "id": "QZwlhy2Y3Sxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para Crear la Base de Datos Vectorial (FAISS)\n",
        "def crea_vectores(documentos, embedding_model_name):\n",
        "\n",
        "  CHUNK_SIZE = 1500\n",
        "  CHUNK_OVERLAP = 500\n",
        "  documentos = []\n",
        "\n",
        "  if os.path.exists(directorio_documentos):\n",
        "\n",
        "    # Obtenemos la lista de los documentos\n",
        "    archivos = sorted([f for f in os.listdir(directorio_documentos) if f.endswith('.txt')])\n",
        "\n",
        "    if archivos:\n",
        "        for i, nombre_archivo in enumerate(archivos):\n",
        "            ruta_documentos = os.path.join(directorio_documentos, nombre_archivo)\n",
        "\n",
        "            print(f\"Cargando documento de: \\033[36m{ruta_documentos}\\033[0m\")\n",
        "            loader = TextLoader(ruta_documentos, encoding=\"utf-8\")\n",
        "            documento_cargado = loader.load()\n",
        "\n",
        "            documentos.extend(documento_cargado)\n",
        "\n",
        "            # Dividir el texto en chunks\n",
        "            # chunk_size y chunk_overlap son importantes para el rendimiento y la calidad\n",
        "\n",
        "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "            docs = text_splitter.split_documents(documentos)\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️ No se encontraron los documentos en el directorio.\")\n",
        "  else:\n",
        "    raise FileNotFoundError(f\"❌ El directorio '{directorio_documentos}' no fue encontrado.\")\n",
        "\n",
        "  print(f\"Número de documentos cargados: \\033[36m{len(documentos)}\\033[0m\")\n",
        "  print(f\"Número de chunks creados: \\033[36m{len(docs)}\\033[0m\")\n",
        "\n",
        "  # Inicializar el modelo de embeddings\n",
        "  print(f\"Cargando modelo de embeddings: \\033[36m{embedding_model_name}\\033[0m\")\n",
        "  # Langchain tiene un wrapper para HuggingFaceEmbeddings\n",
        "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "  # Crear la base de datos vectorial FAISS\n",
        "  print(\"Creando y poblando la base de datos vectorial FAISS\")\n",
        "  vector_store = FAISS.from_documents(docs, embeddings)\n",
        "  print(\"Base de datos vectorial \\033[32mcreada con éxito\\033[0m.\")\n",
        "\n",
        "  return vector_store, embeddings"
      ],
      "metadata": {
        "id": "ZWKdWeWfakXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creación del modelo LLM"
      ],
      "metadata": {
        "id": "PPOVwpSJCU1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos un modelo Large Language Model (LLM) optimizado para eficiencia y compatibilidad con entornos limitados de GPU. La estrategia se basa en la cuantización de 4 bits, que reduce el consumo de memoria sin afectar en gran medida la calidad de las respuestas (Dettmers et al., 2023). Para ello implementamos los siguientes pasos:\n",
        "\n",
        "1️⃣Carga el modelo desde la biblioteca Hugging Face, permitiendo el uso de modelos preentrenados. Esto nos deja  aprovechar las capacidades lingüísticas ya desarrolladas sin necesidad de entrenar desde cero.\n",
        "\n",
        "2️⃣Optimiza el uso de memoria en GPU Mediante la integración de BitsAndBytesConfig y la biblioteca bitsandbytes, configuramos load_in_4bit=True y bnb_4bit_quant_type=\"nf4\" para representar los pesos del modelo en solo 4 bits. Esta reducción de precisión disminuye el uso de VRAM hasta en un 75 %, manteniendo un rendimiento casi idéntico (Hu et al., 2023; Hugging Face, 2024).\n",
        "\n",
        "3️⃣Detecta la disponibilidad de una GPU y carga el modelo en GPU si está disponible; de lo contrario, lo carga en CPU. Este paso garantiza escalabilidad y portabilidad en distintos entornos.\n",
        "\n",
        "4️⃣Configura el tokenizador para la generación de texto, asegurando un procesamiento adecuado de las entradas en asegurando el encoding/decoding adecuado de texto en español, incluyendo manejo de acentos y tokens especiales.\n",
        "\n",
        "**Referencias**\n",
        "\n",
        "* Dettmers, T., et al. (2023). Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA. Hugging Face Blog. Recuperado el 23 de junio de 2025 de https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
        "\n",
        "* Hugging Face. (2024). Quantization [Documentación Transformers]. Recuperado el 23 de junio de 2025 de https://huggingface.co/docs/transformers/main_classes/quantization/"
      ],
      "metadata": {
        "id": "cIvY9jCztp6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def carga_llm(model_name):\n",
        "\n",
        "    # Configuración de cuantización (para ahorrar VRAM en GPU)\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=False,\n",
        "    )\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU detectada. Cargando \\033[36m{model_name}\\033[0m\")\n",
        "\n",
        "        # Cargar tokenizador\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        tokenizer.pad_token = tokenizer.eos_token # Importante para generación\n",
        "\n",
        "        # Cargar modelo con cuantización\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\" # Para distribuir el modelo automáticamente en las GPUs disponibles\n",
        "        )\n",
        "        print(f\"Modelo \\033[36m{model_name}\\033[0m cargado en \\033[35mGPU\\033[0m.\")\n",
        "        return tokenizer, model\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        print(f\"Modelo \\033[36m{model_name}\\033[0m cargado en CPU.\")\n",
        "        return tokenizer, model"
      ],
      "metadata": {
        "id": "-xj8qO9fi9by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creación de la base de datos vectorial y carga de LLM"
      ],
      "metadata": {
        "id": "CZevwwekCcmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta sección, primero construimos la base de recuperación semántica indexando los documentos mediante embeddings y FAISS, lo que permite búsquedas rápidas y relevantes en grandes volúmenes documentales (Douze et al., 2024).\n",
        "\n",
        "Luego, cargamos un LLM preentrenado desde Hugging Face, cuantizado en 4 bits para optimizar el uso de memoria sin perder calidad del significado (Dettmers et al., 2023).\n",
        "\n",
        "**Referencias**\n",
        "* Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré, P.-E., et al. (2024). The Faiss library. arXiv. Recuperado el 23 de junio de 2025 de https://doi.org/10.48550/arXiv.2401.08281\n",
        "* Dettmers, T., et al. (2023). Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA. Hugging Face Blog. Recuperado el 23 de junio de 2025 de https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
        "\n"
      ],
      "metadata": {
        "id": "ZxgeBN8qUc9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear la base de datos vectorial\n",
        "vector_store, embeddings_model = crea_vectores(directorio_documentos, EMBEDDING_MODEL_NAME)\n",
        "# Cargar el LLM\n",
        "tokenizer, llm_model = carga_llm(LLM_MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925,
          "referenced_widgets": [
            "6108ce83ad1b4249821c901665018d27",
            "4dfb48f714264cb297fc702ecf4b5ecd",
            "c811e59d30ef4988b1e432e72389fab0",
            "875ba73f53384554a3dfe59d820258c8",
            "486048964eda45d781110d36b9fe339b",
            "7cce340849e84e968bcea6646a989f6a",
            "c31d15c6a562403696d2c7bdefa7a825",
            "c84e73d908fd4f7e88cd69d5e353d173",
            "aed1c3ff8c29481080b5fcf2123e474f",
            "1f444f7be910437f9f565194021f58ec",
            "372ca5ef171947c9944dd6f7b525d7e9"
          ]
        },
        "id": "croT_E256HcQ",
        "outputId": "c5c1a855-dbe8-4d47-dbd1-331a54942922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9000-002-002.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9000-002-003.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-101.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-105.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-112.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-114.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-115.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-124.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-125.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-126.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-200.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-201.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-202.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-209.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-210.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-211.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-224.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-601.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-604.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-606.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-607.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-611.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-612.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-101.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-102.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-132.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-202.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-205.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-206.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-208.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-209.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-210.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-211.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-213.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-214.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-008-101.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-008-203.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-008-204.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-018-200.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/Filtros.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/LSS.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/Procedimiento Alta Patronal.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/Reg_LSS_MACERF.txt\u001b[0m\n",
            "Número de documentos cargados: \u001b[36m43\u001b[0m\n",
            "Número de chunks creados: \u001b[36m2260\u001b[0m\n",
            "Cargando modelo de embeddings: \u001b[36msentence-transformers/all-MiniLM-L6-v2\u001b[0m\n",
            "Creando y poblando la base de datos vectorial FAISS\n",
            "Base de datos vectorial \u001b[32mcreada con éxito\u001b[0m.\n",
            "GPU detectada. Cargando \u001b[36mmistralai/Mistral-7B-Instruct-v0.2\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6108ce83ad1b4249821c901665018d27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo \u001b[36mmistralai/Mistral-7B-Instruct-v0.2\u001b[0m cargado en \u001b[35mGPU\u001b[0m.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creación del pipeline"
      ],
      "metadata": {
        "id": "mCq03MIwCguq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuramos el pipeline de generación de texto basado modelo LLM preentrenado (LLM) utilizando la clase pipeline de Hugging Face Transformers. Este pipeline se encarga de recibir las consultas del usuario, procesarlas mediante el tokenizador y generar una respuesta coherente, fundamentada en la información recuperada por el sistema RAG. (HuggingFace, 2024)\n",
        "\n",
        "Esta etapa es esencial en el funcionamiento del chatbot, ya que define cómo el modelo genera las respuestas, incluyendo parámetros que afectan su longitud, creatividad y precisión semántica. A continuación definimos estos parámetros:\n",
        "\n",
        "⚙️ Parámetros principales\n",
        "- `model=llm_model`: especifica el modelo de lenguaje preentrenado que se utilizará.\n",
        "- `tokenizer=tokenizer`: define el tokenizador asociado al modelo, responsable de convertir texto en tokens y viceversa.\n",
        "- `max_new_tokens=800`: establece la longitud máxima de texto generado, en número de tokens.\n",
        "- `do_sample=True`: habilita el muestreo aleatorio en la generación de texto, en lugar de elegir siempre la opción más probable.\n",
        "- `temperature=0.8`: ajusta el nivel de creatividad; valores más altos generan respuestas más variadas.\n",
        "- `top_k=50`: limita la selección a los 50 tokens más probables antes de elegir aleatoriamente.\n",
        "- `top_p=0.95`: aplica “nucleus sampling”, seleccionando entre los tokens con una probabilidad acumulada del 95 %.\n",
        "- `eos_token_id=tokenizer.eos_token_id`: define el token de fin de secuencia para detener la generación cuando se alcanza.\n",
        "- `return_full_text=False`: indica que se debe devolver solo el texto generado, excluyendo el texto de entrada.\n",
        "\n",
        "**Referencias**\n",
        "* Hugging Face. (2024). Text Generation Pipelines. Recuperado el 23 de junio de 2025 de https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.TextGenerationPipeline"
      ],
      "metadata": {
        "id": "UFw6eLaZ0jt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el pipeline de HuggingFace para el LLM\n",
        "# Se especifica temperature para controlar la creatividad y max_new_tokens para limitar la longitud de la respuesta\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llm_model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=1500, # Longitud máxima de la respuesta generada\n",
        "    do_sample=True,\n",
        "    temperature=0.8, # Creatividad del modelo (0.0 a 1.0, más alto es más creativo)\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    eos_token_id=tokenizer.eos_token_id, # Asegura que el modelo se detenga al generar el token EOS\n",
        "    return_full_text = False\n",
        ")"
      ],
      "metadata": {
        "id": "SxG5PU0etiHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b81e0309-e2d5-475f-9a43-a8dfd28bc88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Langchain para responder preguntas de forma contextualizada"
      ],
      "metadata": {
        "id": "U8Z7HWOiCnyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este framework permite ejecutar fácilmente el flujo entre los componentes del sistema: recuperación, procesamiento y generación de texto (Zhou et al., 2023). En este pipeline, la consulta del usuario activa los siguientes pasos:\n",
        "\n",
        "1️⃣ **Integración del modelo de lenguaje (LLM)**\n",
        "\n",
        "El modelo de lenguaje preentrenado, cargado desde Hugging Face y previamente cuantizado para eficiencia, se conecta mediante HuggingFacePipeline. Esto permite usar el mismo modelo con parámetros de control definidos en la etapa anterior (como temperatura, top_k, etc.).\n",
        "\n",
        "2️⃣ **Recuperador semántico (Retriever)**\n",
        "\n",
        "Se crea un retriever a partir del índice vectorial FAISS, que permite recuperar los fragmentos de texto más relevantes para una consulta dada. Este componente se encarga de realizar la búsqueda semántica dentro del corpus documental del IMSS, usando comparaciones vectoriales.\n",
        "\n",
        "3️⃣ **Prompt personalizado**\n",
        "\n",
        "Se define un prompt instructivo que guía al modelo para:\n",
        "* Ser claro y amigable.\n",
        "* Evitar generar respuestas sin fundamento (“alucinaciones”).\n",
        "* Basarse únicamente en los documentos proporcionados.\n",
        "* El prompt representa una capa crítica de control, alineando el comportamiento del LLM con la ética y objetivos del chatbot (Zhou et al., 2023).\n",
        "\n",
        "4️⃣ **Construcción de la cadena RAG**\n",
        "Se construye una cadena de tipo RetrievalQA en LangChain, que:\n",
        "\n",
        "* Toma como entrada la pregunta del usuario.\n",
        "* Recupera fragmentos relevantes del corpus mediante FAISS.\n",
        "* Genera una respuesta a partir de los documentos recuperados.\n",
        "* Devuelve una salida textual clara, fundamentada y útil.\n",
        "\n",
        "Este flujo cierra el ciclo RAG: recuperación de contexto + generación controlada, optimizando la calidad de las respuestas sin necesidad de entrenamiento adicional del modelo.\n",
        "\n",
        "**Referencias**\n",
        "* Zhou, M., et al. (2023). DocPrompting: Generating documents with retrieval-augmented language models. Proceedings of the 2023 Conference of the Association for Computational Linguistics. https://arxiv.org/abs/2305.06983\n",
        "\n"
      ],
      "metadata": {
        "id": "WUphf0TDWpK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente código, implementa un flujo completo basado en RAG (Retrieval-Augmented Generation) utilizando LangChain para responder preguntas de forma contextualizada.\n",
        "- Configura un modelo de lenguaje (LLM) a través de Hugging Face mediante `HuggingFacePipeline`.\n",
        "- Crea un retriever que obtiene fragmentos de información relevantes desde una base de datos vectorial.\n",
        "- Define un prompt personalizado que orienta al modelo para responder con amabilidad, precisión y sin generar información inventada.\n",
        "- Construye una cadena RAG que:\n",
        "- Recupera información contextual.\n",
        "- Genera una respuesta basada en dicha información.\n",
        "- Devuelve una salida textual clara y útil."
      ],
      "metadata": {
        "id": "WmdPl-X92iM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "\n",
        "# Crear el retriever\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 6}) # Recupera los 6 chunks más relevantes\n",
        "\n",
        "# --- Definir el Prompt Template ---\n",
        "# Este prompt guía al LLM sobre cómo debe responder\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "    Eres un asistente de IA amigable y eficiente que responde preguntas relacionadas con procedimientos de afiliación.\n",
        "    Debes responder únicamente con base en el contexto proporcionado, sin mencionar los documentos ni fuentes consultadas.\n",
        "    Si la información del contexto no es suficiente para responder con certeza, indica amablemente que no estás seguro.\n",
        "    Cada respuesta debe comenzar con una frase introductoria distinta que transmita amabilidad, interés y cercanía (por ejemplo: \"Con gusto te explico...\" o \"Qué buena pregunta, aquí tienes la información...\"), y procura variar esa frase en cada ocasión.\n",
        "    Al final de cada respuesta, incluye:\n",
        "    – Una frase que invite al usuario a contactar al área normativa en caso de dudas adicionales (por ejemplo: \"Si tienes alguna otra duda, no dudes en ponerte en contacto con el área normativa correspondiente.\")\n",
        "    – Una advertencia clara indicando que el contenido fue generado por inteligencia artificial, que puede contener errores, y que es importante verificar la información antes de tomar decisiones.\n",
        "\n",
        "\n",
        "    Contexto: {context}\n",
        "\n",
        "    Pregunta: {question}\n",
        "\n",
        "    Respuesta útil:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# --- Construir la cadena de RAG con Langchain Expression Language (LCEL) ---\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt_template\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "DL508LcWr_gE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b71ad590-1bac-498f-af70-4121f12b3f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_6292/1381692076.py:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=llm_pipeline)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot"
      ],
      "metadata": {
        "id": "mIWmsYSwCxri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función `chat_with_rag` recibe una pregunta del usuario y devuelve una respuesta generada a partir de la cadena RAG, la cual combina recuperación de información y generación de texto.\n",
        "\n",
        "Se crea una aplicación web interactiva que permite:\n",
        "- Ingresar preguntas en un campo de texto.\n",
        "- Obtener respuestas generadas por IA basadas en información recuperada.\n",
        "- Visualizar un título, una descripción y ejemplos que facilitan la interacción del usuario.\n",
        "\n",
        "La creación de la interfaz incluye las siguientes configuraciones clave:\n",
        "- `fn=chat_with_rag`: conecta la función principal del asistente con la interfaz.\n",
        "- `inputs`: define el campo de entrada donde el usuario formula su pregunta.\n",
        "- `outputs`: especifica el área donde se muestra la respuesta generada.\n",
        "- `title`: establece el título visible de la aplicación.\n",
        "- `description`: proporciona una breve explicación sobre el propósito de la interfaz.\n",
        "- `examples`: muestra preguntas sugeridas que el usuario puede seleccionar para probar el sistema.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uYbcvV_N3yb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Función para la interfaz de Gradio ---\n",
        "def chat_with_rag(user_query):\n",
        "    try:\n",
        "        response = rag_chain.invoke(user_query)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"Ocurrió un error: {e}\"\n",
        "\n",
        "# --- Interfaz de Gradio ---\n",
        "print(\"Creando interfaz de Gradio...\")\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=chat_with_rag,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Escribe tu pregunta aquí...\", label=\"Tu Pregunta\"),\n",
        "    outputs=gr.Textbox(label=\"Respuesta del Asistente\"),\n",
        "    title=\"Consultor Virtual de la Unidad de Incorporación al Seguro Social – Potenciado por IA\",\n",
        "    description=\"Aclaramos tus consultas sobre los procedimientos de afiliación.\",\n",
        "    examples=[\"Explícame el procedimiento de alta patronal\", \"¿Qué modalidades validas existen?\", \"¿Cómo doy de alta un patron persona física?\"]\n",
        ")\n",
        "\n",
        "# Lanzar la interfaz\n",
        "iface.launch(share=True) # share=True para generar un enlace público temporal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "t7r5Rf4wsMPC",
        "outputId": "35c314fc-b0ae-4d6a-82c9-e958dab2656b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creando interfaz de Gradio...\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://62e9020ce8a6345c57.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://62e9020ce8a6345c57.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusiones:**"
      ],
      "metadata": {
        "id": "Kx-dZSFJz9cK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### **Incluyan sus conclusiones de la actividad chatbot LLM + RAG:**\n",
        "\n",
        "\n",
        "\n",
        "None"
      ],
      "metadata": {
        "id": "3w3usdaC0BCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fin de la actividad chatbot: LLM + RAG**"
      ],
      "metadata": {
        "id": "CtB5Q3m41YQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revisar si queremos formato markdown"
      ],
      "metadata": {
        "id": "63OXBodwLGyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://faiss.ai/"
      ],
      "metadata": {
        "id": "WdbER9uis7c7"
      }
    }
  ]
}