{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state":{},
        "6108ce83ad1b4249821c901665018d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4dfb48f714264cb297fc702ecf4b5ecd",
              "IPY_MODEL_c811e59d30ef4988b1e432e72389fab0",
              "IPY_MODEL_875ba73f53384554a3dfe59d820258c8"
            ],
            "layout": "IPY_MODEL_486048964eda45d781110d36b9fe339b",
            "tabbable": null,
            "tooltip": null
          }
        },
        "4dfb48f714264cb297fc702ecf4b5ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_7cce340849e84e968bcea6646a989f6a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c31d15c6a562403696d2c7bdefa7a825",
            "tabbable": null,
            "tooltip": null,
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "c811e59d30ef4988b1e432e72389fab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_c84e73d908fd4f7e88cd69d5e353d173",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aed1c3ff8c29481080b5fcf2123e474f",
            "tabbable": null,
            "tooltip": null,
            "value": 3
          }
        },
        "875ba73f53384554a3dfe59d820258c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_1f444f7be910437f9f565194021f58ec",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_372ca5ef171947c9944dd6f7b525d7e9",
            "tabbable": null,
            "tooltip": null,
            "value": "‚Äá3/3‚Äá[00:05&lt;00:00,‚Äá‚Äá1.70s/it]"
          }
        },
        "486048964eda45d781110d36b9fe339b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cce340849e84e968bcea6646a989f6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c31d15c6a562403696d2c7bdefa7a825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "c84e73d908fd4f7e88cd69d5e353d173": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed1c3ff8c29481080b5fcf2123e474f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f444f7be910437f9f565194021f58ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372ca5ef171947c9944dd6f7b525d7e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## Maestr√≠a en Inteligencia Artificial Aplicada\n",
        "#### Tecnol√≥gico de Monterrey\n",
        "#### Prof Luis Eduardo Falc√≥n Morales\n",
        "\n",
        "### **Adtividad en Equipos: sistema LLM + RAG**"
      ],
      "metadata": {
        "id": "-hVND8xY2OKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Equipo 18**\n",
        "> ### üë®‚Äçüíª **Juan Carlos P√©rez Nava**\n",
        "> `A01795941`\n",
        ">\n",
        "> ### üë®‚Äçüíª **Javier Augusto Rebull Saucedo**\n",
        "> `A01795838`\n",
        ">\n",
        "> ### üë©‚Äçüíª **Sihin√≠ Trinidad S√°nchez**\n",
        "> `A00889358`\n",
        ">\n",
        "> ### üë©‚Äçüíª **Iris Monserrat Urbina Casas**\n",
        "> `A01795999`\n",
        "\n"
      ],
      "metadata": {
        "id": "aimHVFOv23lm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ##### **El formato de este cuaderno de Jupyter es libre, pero debe incuir al menos las siguientes secciones:**\n",
        "\n",
        "  * ##### **Introducci√≥n de la problem√°tica a resolver.**\n",
        "  * ##### **Sistema RAG + LLM**\n",
        "  * ##### **El chatbot, incluyendo ejemplos de prueba.**\n",
        "  * ##### **Conclusiones**\n",
        "\n",
        "* ##### **Pueden importar los paquetes o librer√≠as que requieran.**\n",
        "\n",
        "* ##### **Pueden incluir las celdas y l√≠neas de c√≥digo que deseen.**"
      ],
      "metadata": {
        "id": "7jimvsiVgjMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducci√≥n de la problem√°tica a resolver\n",
        "\n",
        "En las √°reas operativas del Instituto Mexicano del Seguro Social existen diversas oportunidades de mejora en la atenci√≥n al p√∫blico, la resoluci√≥n de problemas y el entendimiento del negocio y sus procedimientos. Por ello, se ha dise√±ado la integraci√≥n de normas, reglas y procedimientos espec√≠ficos para la atenci√≥n en cada una de las ventanillas de la Direcci√≥n de Incorporaci√≥n y Recaudaci√≥n, encargada de recibir las solicitudes de registro de los patrones y sus trabajadores, as√≠ como de gestionar la recaudaci√≥n de las aportaciones patronales. Sin embargo, identificamos como problem√°tica la dificultad que enfrentan muchas personas para entender los requisitos, documentos, horarios, modalidades, etc. Aunque la informaci√≥n oficial est√° disponible en l√≠nea, suele estar dispersa, fragmentada y en un lenguaje t√©cnico y dif√≠cil de interpretar.\n",
        "\n",
        "\n",
        "Con el objetivo de mejorar el acceso a esta informaci√≥n y optimizar la experiencia del usuario, desarrollamos un chatbot basado en un modelo de lenguaje de gran tama√±o (LLM), complementado con un sistema de Recuperaci√≥n Aumentada Generativa (RAG). Esta arquitectura permite generar respuestas precisas y contextualizadas a partir de documentos reales del IMSS, mejorando as√≠ la fidelidad factual del modelo (Lewis et al., 2020).\n",
        "\n",
        "\n",
        "El corpus utilizado fue construido a partir de documentos recuperados de la secci√≥n oficial de ‚ÄúTr√°mites y servicios‚Äù del IMSS (2025), los cuales explican procesos clave como el alta patronal y la gesti√≥n de incidencias en el sistema. Estos documentos contienen instrucciones detalladas que garantizan que cada situaci√≥n pueda ser atendida de forma eficiente y conforme a la normatividad vigente.\n",
        "\n",
        "\n",
        "Este chatbot busca ser una herramienta de apoyo para ciudadanos, empresarios y trabajadores, facilitando la comprensi√≥n de los procedimientos institucionales y apoyando la resoluci√≥n aut√≥noma de dudas durante la gesti√≥n de sus tr√°mites.\n",
        "\n",
        "\n",
        "**Referencias:**\n",
        "\n",
        "* Lewis, M., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. arXiv preprint arXiv:2005.11401 Recuperado el 20 de junio de 2025. https://arxiv.org/abs/2005.11401\n",
        "\n",
        "* IMSS. (2025). Tr√°mites y servicios. Recuperado el 22 de junio de 2025. https://www.imss.gob.mx/tramites\n"
      ],
      "metadata": {
        "id": "WVw8V0cBrFm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sistema RAG + LLM"
      ],
      "metadata": {
        "id": "4EY70nzmrHl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el contexto del desarrollo de nuestro chatbot para tr√°mites del IMSS, implementamos arquitectura de Generaci√≥n Aumentada por Recuperaci√≥n (RAG), un enfoque que combina la capacidad generativa de los modelos de lenguaje (LLM) con mecanismos de recuperaci√≥n de documentos relevantes. Como lo aprendimos durante el curso y la actividad de IBM de Generative AI in Action, esta arquitectura ha demostrado mejorar la precisi√≥n, actualidad y fiabilidad de las respuestas generadas (AWS, 2025; IBM, 2025; Lewis et al., 2020).\n",
        "\n",
        "\n",
        "**¬øC√≥mo funciona RAG?**\n",
        "\n",
        "AWS (2025) RAG explica que RAG incorpora un mecanismo de recuperaci√≥n de informaci√≥n antes de generar una respuesta, lo que permite que el modelo se base en datos actualizados y verificables. El LLM recibe una pregunta del usuario y antes de responder, consulta un repositorio de informaci√≥n actualizado, que puede ser una fuente abierta como la web o una base de datos espec√≠fica. Despu√©s, genera la respuesta basada en los datos recuperados, proporcionando evidencia o fuentes verificables para respaldar su contenido.\n",
        "\n",
        "\n",
        "En nuestro chatbot, el sistema RAG opera en tres fases principales:\n",
        "1. Recepci√≥n de la consulta del usuario.\n",
        "2. Recuperaci√≥n sem√°ntica de fragmentos de documentos relevantes usando un √≠ndice vectorial construido previamente a partir de fuentes oficiales del IMSS.\n",
        "3. Generaci√≥n de una respuesta contextualizada, donde el modelo de lenguaje integra la informaci√≥n recuperada para formular una respuesta coherente y fundamentada.\n",
        "\n",
        "\n",
        "Este enfoque permite que el chatbot no dependa exclusivamente de conocimiento preentrenado (potencialmente desactualizado), sino que acceda en tiempo real a documentaci√≥n actualizada y espec√≠fica del dominio institucional.\n",
        "\n",
        "\n",
        "**Beneficios (IBM, 2025; Lewis, 2020)**\n",
        "\n",
        "\n",
        "* Mayor precisi√≥n y actualidad: Evita respuestas\n",
        "err√≥neas o desactualizadas al basarse en informaci√≥n reciente.\n",
        "* Reducci√≥n de \"alucinaciones\": Minimiza la generaci√≥n de datos inventados por el modelo.\n",
        "* Mayor confiabilidad: Permite que el modelo reconozca cuando no tiene una respuesta confiable, evitando la generaci√≥n de informaci√≥n incorrecta\n",
        "\n",
        "\n",
        "**Advertencias**\n",
        "\n",
        "Para que RAG funcione de manera √≥ptima, el sistema de recuperaci√≥n debe ser eficiente y proporcionar datos relevantes y de alta calidad. Si la informaci√≥n recuperada es insuficiente o poco precisa, el modelo podr√≠a no responder correctamente, incluso si la respuesta existe en alguna fuente confiable. (Como todo en IA, \"garbage in, garbage out!\"  Por ello, seleccionamos documentos directamente de fuentes oficiales del IMSS como corpus para garantizar su relevancia y validez.\n",
        "\n",
        "\n",
        "**Especificaciones del RAG implementado**\n",
        "\n",
        "Para nuestra soluci√≥n, elegimos los siguientes componentes t√©cnicos:\n",
        "\n",
        "* Modelo de lenguaje grande (LLM): \"mistralai/Mistral-7B-Instruct-v0.2.\"\n",
        "* Embeddings: HuggingFaceEmbeddings con modelo multiling√ºe (sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2), ideal para recuperar contenido en espa√±ol.\n",
        "* Divisi√≥n del corpus: Mediante RecursiveCharacterTextSplitter, con chunk_size=1500 y chunk_overlap=500, para conservar el contexto entre fragmentos.\n",
        "* Almacenamiento vectorial: FAISS como √≠ndice de recuperaci√≥n, por su eficiencia en b√∫squeda sem√°ntica.\n",
        "* Pipeline de recuperaci√≥n: LangChain, configurado para buscar los topk=50fragmentos m√°s relevantes por consulta.\n",
        "\n",
        "\n",
        "**Referencias**\n",
        "\n",
        "* AWS. (2025). What is retrieval augmented generation? Recuperado el 15 de junio de 2025 de https://aws.amazon.com/es/what-is/retrieval-augmented-generation/\n",
        "\n",
        "* IBM. (2025). Generative AI in Action. IBM SkillsLab. Recuperado el 15 de junio de 2025 de https://www.ibm.com/academic/topic/artificial-intelligence?ach_id=9217fca4-c36f-4e1d-ab87-da43469344a5\n",
        "\n",
        "* ewis, M., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. arXiv preprint arXiv:2005.11401. https://arxiv.org/abs/2005.11401"
      ],
      "metadata": {
        "id": "KFFxZqDxtPWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalaci√≥n de librer√≠as"
      ],
      "metadata": {
        "id": "NDXUlSJHBx3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Instalamos las librer√≠as necesarias para construir un chatbot RAG con LLM, recuperaci√≥n sem√°ntica, procesamiento de PDFs e interfaz con Gradio.\n",
        "!pip install torch transformers sentence-transformers faiss-cpu langchain pypdf accelerate bitsandbytes langchain-community gradio PyMuPDF"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JgI1rNFOZw_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f3fcb68-240a-4c04-c89d-db338ac6c4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in ./miniconda3/envs/torch/lib/python3.12/site-packages (2.8.0.dev20250610+cu128)\r\n",
            "Requirement already satisfied: transformers in ./miniconda3/envs/torch/lib/python3.12/site-packages (4.52.4)\r\n",
            "Requirement already satisfied: sentence-transformers in ./miniconda3/envs/torch/lib/python3.12/site-packages (4.1.0)\r\n",
            "Requirement already satisfied: faiss-cpu in ./miniconda3/envs/torch/lib/python3.12/site-packages (1.11.0)\r\n",
            "Requirement already satisfied: langchain in ./miniconda3/envs/torch/lib/python3.12/site-packages (0.3.25)\r\n",
            "Requirement already satisfied: pypdf in ./miniconda3/envs/torch/lib/python3.12/site-packages (5.6.0)\r\n",
            "Requirement already satisfied: accelerate in ./miniconda3/envs/torch/lib/python3.12/site-packages (1.7.0)\r\n",
            "Requirement already satisfied: bitsandbytes in ./miniconda3/envs/torch/lib/python3.12/site-packages (0.46.0)\r\n",
            "Requirement already satisfied: langchain-community in ./miniconda3/envs/torch/lib/python3.12/site-packages (0.3.25)\r\n",
            "Requirement already satisfied: gradio in ./miniconda3/envs/torch/lib/python3.12/site-packages (5.33.1)\r\n",
            "Requirement already satisfied: PyMuPDF in ./miniconda3/envs/torch/lib/python3.12/site-packages (1.26.1)\r\n",
            "Requirement already satisfied: filelock in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (3.18.0)\r\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (4.14.0)\r\n",
            "Requirement already satisfied: setuptools in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (78.1.1)\r\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (1.14.0)\r\n",
            "Requirement already satisfied: networkx in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (3.5)\r\n",
            "Requirement already satisfied: jinja2 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (3.1.6)\r\n",
            "Requirement already satisfied: fsspec in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (2025.5.1)\r\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (12.8.93)\r\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (12.8.90)\r\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (12.8.90)\r\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (9.8.0.87)\r\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (12.8.4.1)\r\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (11.3.3.83)\r\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (10.3.9.90)\r\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (11.7.3.90)\r\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (12.5.8.93)\r\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (0.7.1)\r\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.5 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (2.26.5)\r\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.2.5 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (3.2.5)\r\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (12.8.90)\r\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (12.8.93)\r\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (1.13.1.3)\r\n",
            "Requirement already satisfied: pytorch-triton==3.3.1+gitc8757738 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from torch) (3.3.1+gitc8757738)\r\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (0.32.5)\r\n",
            "Requirement already satisfied: numpy>=1.17 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (2.3.0)\r\n",
            "Requirement already satisfied: packaging>=20.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (24.2)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (6.0.2)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (2024.11.6)\r\n",
            "Requirement already satisfied: requests in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (2.32.4)\r\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (0.21.1)\r\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (0.5.3)\r\n",
            "Requirement already satisfied: tqdm>=4.27 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: scikit-learn in ./miniconda3/envs/torch/lib/python3.12/site-packages (from sentence-transformers) (1.7.0)\n",
            "Requirement already satisfied: scipy in ./miniconda3/envs/torch/lib/python3.12/site-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in ./miniconda3/envs/torch/lib/python3.12/site-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio in ./miniconda3/envs/torch/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: certifi in ./miniconda3/envs/torch/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in ./miniconda3/envs/torch/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in ./miniconda3/envs/torch/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: psutil in ./miniconda3/envs/torch/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain-community) (3.12.12)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.3 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (1.10.3)\n",
            "Requirement already satisfied: groovy~=0.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (2.3.0)\n",
            "Requirement already satisfied: pydub in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.11.13)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from gradio-client==1.10.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: click>=8.0.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
            "Requirement already satisfied: six>=1.5 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./miniconda3/envs/torch/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Librer√≠as para Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "Estas bibliotecas permiten cargar documentos, dividirlos en fragmentos y representarlos como vectores para una b√∫squeda eficiente.\n",
        "- `TextLoader`: Carga documentos de texto.\n",
        "- `CharacterTextSplitter` y `RecursiveCharacterTextSplitter`: Dividen los documentos en fragmentos m√°s peque√±os para su procesamiento.\n",
        "- `HuggingFaceEmbeddings`: Convierte texto en representaciones vectoriales mediante modelos de embeddings.\n",
        "- `FAISS`: Motor de b√∫squeda basado en √≠ndices de vectores, dise√±ado para recuperar informaci√≥n de manera r√°pida y eficiente.\n",
        "\n",
        "üîπ Librer√≠as para el Modelo de Lenguaje Grande (LLM)\n",
        "\n",
        "Estas herramientas permiten generar respuestas utilizando modelos de lenguaje basados en Transformers.\n",
        "- `AutoModelForCausalLM` y `AutoTokenizer`: Cargan y configuran el modelo de generaci√≥n de texto.\n",
        "- `BitsAndBytesConfig`: Optimiza la ejecuci√≥n del modelo para reducir el consumo de memoria.\n",
        "- `pipeline`: Establece un flujo de procesamiento que facilita la generaci√≥n de texto.\n",
        "- `torch`: Biblioteca de aprendizaje profundo utilizada para el manejo y ejecuci√≥n de modelos de lenguaje.\n",
        "\n"
      ],
      "metadata": {
        "id": "T_YbgETuz5d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importaciones para el sistema RAG (Retrieval-Augmented Generation)\n",
        "from langchain.document_loaders import TextLoader  # Carga documentos de texto\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter  # Divide el texto en fragmentos para facilitar el procesamiento\n",
        "from langchain_huggingface import HuggingFaceEmbeddings  # Para generar vectores de texto con modelos de Hugging Face\n",
        "from langchain_community.vectorstores import FAISS  # FAISS para almacenamiento y b√∫squeda eficiente de vectores\n",
        "\n",
        "# Importaciones para el modelo LLM (Generaci√≥n de respuestas)\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline  # Para cargar y utilizar modelos de lenguaje\n",
        "import torch  # Biblioteca para operaciones con tensores y aceleraci√≥n en GPU\n",
        "\n",
        "# Autenticaci√≥n con Hugging Face Hub para acceder a modelos\n",
        "from huggingface_hub import login\n",
        "login(token=\"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\")\n",
        "\n",
        "# Elementos para construir la cadena RAG con Langchain\n",
        "from langchain.prompts import PromptTemplate  # Plantilla de prompt para el modelo de lenguaje\n",
        "from langchain.schema.runnable import RunnablePassthrough  # Permite que una funci√≥n pase datos sin modificarlos\n",
        "from langchain_community.llms import HuggingFacePipeline  # Integra modelos de Hugging Face como LLM en Langchain\n",
        "from langchain.schema import StrOutputParser  # Analiza salidas del modelo como texto plano\n",
        "\n",
        "# Librer√≠a Gradio para construir interfaces web interactivas\n",
        "import gradio as gr\n",
        "\n",
        "# M√≥dulos auxiliares para rutas, expresiones regulares, descarga de archivos y lectura de PDFs\n",
        "import os  # Manejo de rutas de archivos y directorios\n",
        "import re  # Manejo de expresiones regulares\n",
        "import gdown  # Descarga de archivos desde Google Drive\n",
        "import fitz  # Lectura y manipulaci√≥n de documentos PDF (PyMuPDF)\n",
        "\n",
        "# Definici√≥n del modelo de embeddings\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Definici√≥n del modelo de lenguaje grande (LLM)\n",
        "LLM_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# Rutas y configuraciones para los documentos\n",
        "directorio_documentos = 'datos_RAG_LLM'  # Carpeta donde se almacenar√°n los documentos para el sistema RAG\n",
        "id_repositorio = '18z_pJe7EcZO6oqCSebqS_Qe-E1kL4RUH'  # ID de Google Drive para descargar los documentos\n",
        "\n",
        "# Mensajes de confirmaci√≥n para mostrar que las librer√≠as y rutas han sido configuradas correctamente\n",
        "print(\"\\033[32mLibrer√≠as importadas:\\033[0m\")\n",
        "print(f\"Modelo de Embeddings: \\033[36m{EMBEDDING_MODEL_NAME}\\033[0m\")\n",
        "print(f\"Modelo LLM: \\033[36m{LLM_MODEL_NAME}\\033[0m\")\n",
        "\n",
        "print(\"\\033[32mRutas configuradas:\\033[0m\")\n",
        "print(f\"Carpeta de documentos: \\033[36m{directorio_documentos}\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe5MFyfVR2uW",
        "outputId": "b95eddce-bf29-4fa3-a78b-1663ed201025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mLibrer√≠as importadas:\u001b[0m\n",
            "Modelo de Embeddings: \u001b[36msentence-transformers/all-MiniLM-L6-v2\u001b[0m\n",
            "Modelo LLM: \u001b[36mmistralai/Mistral-7B-Instruct-v0.2\u001b[0m\n",
            "\u001b[32mRutas configuradas:\u001b[0m\n",
            "Carpeta de documentos: \u001b[36mdatos_RAG_LLM\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparaci√≥n del Corpus Documental"
      ],
      "metadata": {
        "id": "kZVb0NEBMaJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recolecci√≥n de documentos**\n",
        "\n",
        "Para garantizar que el chatbot generara respuestas precisas y contextualizadas, construimos un corpus documental espec√≠fico a partir de fuentes oficiales del Instituto Mexicano del Seguro Social (IMSS), particularmente de su secci√≥n de ‚ÄúTr√°mites y servicios‚Äù (IMSS, 2025). Este corpus contiene informaci√≥n detallada sobre procesos como registro patronal, afiliaci√≥n voluntaria, incapacidades y correcci√≥n de datos, entre otros.\n",
        "\n",
        "Los documentos fueron seleccionados manualmente desde el sitio oficial del IMSS, asegurando su relevancia y actualidad. Se priorizaron gu√≠as, instructivos y fichas de tr√°mites que explican paso a paso cada procedimiento.\n",
        "\n",
        "**Extracci√≥n y limpieza**\n",
        "\n",
        "Los documentos se encontraban en formato PDF, por lo que utilizamos la librer√≠a PyMuPDF (fitz) para extraer su contenido textual. Este m√©todo permite preservar la estructura del texto con mayor fidelidad que otras alternativas como pypdf. Durante esta fase se eliminaron encabezados, pies de p√°gina y p√°ginas en blanco y se normaliz√≥ la codificaci√≥n (UTF-8) y se corrigieron errores de segmentaci√≥n y caracteres extra√±os.\n",
        "\n",
        "**Divisi√≥n del contenido en fragmentos**\n",
        "\n",
        "Una parte esencial del sistema RAG es la divisi√≥n del texto en fragmentos o chunks antes de generar sus vectores. Esta segmentaci√≥n permite construir un √≠ndice de recuperaci√≥n sem√°ntica m√°s efectivo y relevante. En nuestro caso, usamos RecursiveCharacterTextSplitter de LangChain con los par√°metros: chunk_size = 1500 caracteres; chunk_overlap = 500 caracteres\n",
        "\n",
        "Esta configuraci√≥n permite que los fragmentos tengan suficiente contexto para ser √∫tiles durante la recuperaci√≥n, mientras el solapamiento (overlap) evita la p√©rdida de informaci√≥n cr√≠tica al final de cada fragmento.\n",
        "\n",
        "La literatura t√©cnica en modelos RAG enfatiza que una divisi√≥n adecuada del texto mejora la precisi√≥n del sistema de recuperaci√≥n, ya que los modelos de embeddings generan mejores vectores sem√°nticos cuando operan sobre unidades coherentes de contenido (Izacard & Grave, 2020). LangChain sugiere que este enfoque tambi√©n reduce los errores comunes del modelo, como respuestas alucinadas o sin fundamento, al asegurar que la informaci√≥n relevante est√© presente durante la fase de generaci√≥n (LangChain, 2025).\n",
        "\n",
        "**Referencias**\n",
        "* Izacard, G., & Grave, E. (2020). Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. arXiv. https://arxiv.org/abs/2007.01282\n",
        "\n",
        "LangChain. (2025). Text Splitters. Recuperado el 22 de junio de 2025 de https://python.langchain.com/docs/concepts/text_splitters/\n",
        "\n",
        "Hugging Face. (2024). Dealing with Chunked Input Text and Summaries for Fine Tuning Summarization model. Recuperado el 22 de junio de 2025 de https://discuss.huggingface.co/t/dealing-with-chunked-input-text-and-summaries-for-fine-tuning-summarization-model/77186\n",
        "\n"
      ],
      "metadata": {
        "id": "WjsZNzaLMeeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Descargar los documentos para el modelo\n",
        "print(f\"üìÇ Ruta de los documentos ‚Üí \\033[36m'{directorio_documentos}\\033[0m'\\n\")\n",
        "\n",
        "# Crear el directorio si no existe\n",
        "if not os.path.exists(directorio_documentos):\n",
        "    os.makedirs(directorio_documentos)\n",
        "\n",
        "gdown.download_folder(id=id_repositorio, output=directorio_documentos, quiet=False, use_cookies=False)\n",
        "\n",
        "archivos_cargados = os.listdir(directorio_documentos)\n",
        "for archivo in archivos_cargados:\n",
        "    print(f\"üìÇ Documento cargado: \\033[36m{archivo}\\033[0m\")\n",
        "\n",
        "print(f\"Total de documentos descargados: \\033[32m{len(archivos_cargados)}\\033[0m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4ptoOud26iNb",
        "outputId": "71019730-7e06-47c0-e4a5-de630166914f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Ruta de los documentos ‚Üí \u001b[36m'datos_RAG_LLM\u001b[0m'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1q1I1zt_ygtTYjYjKewFq4zf4g0NkxsVA 9000-002-002.pdf\n",
            "Processing file 1a-f4o7ZyuYHeEZBsQ2GVfsY5p4D1Lhwm 9000-002-003.pdf\n",
            "Processing file 1in4iGXlLxQQXeu-3SGeSf7q6Jg80en8t 9210-003-101.pdf\n",
            "Processing file 1KvTvJqYUNMALFxghrxu_ubnxYBoQxeig 9210-003-105.pdf\n",
            "Processing file 1Dx79o2digN-QuVgplrlUrcN_4AUkSMPA 9210-003-112.pdf\n",
            "Processing file 17skF_0icdMo6t2c2rkmfpetvBh482JCy 9210-003-114.pdf\n",
            "Processing file 1fsAN6bJ_giDsiawRj4aTErkke940AhBD 9210-003-115.pdf\n",
            "Processing file 10rXmodDefLNjsAQAom7M-nJQgM9_3mzF 9210-003-124.pdf\n",
            "Processing file 1eF-y19UXMTZ6eO-JauGVG9JxngkAl03j 9210-003-125.pdf\n",
            "Processing file 1_QRQ5wEc2ehj5AvVca3E_j4UWXD7H68r 9210-003-126.pdf\n",
            "Processing file 1QynDJpMW3o04Rq7MjzHUerkzoWHfF-GI 9210-003-200.pdf\n",
            "Processing file 1QAmniZXbTS7xUueXpR_90fEe4wJMJK3_ 9210-003-201.pdf\n",
            "Processing file 12Tf_lk8cztw_qzTAAwElinNTKkfr-QTx 9210-003-202.pdf\n",
            "Processing file 19E3EQ0QkW18nlxS0XHqiPKfjqBoOXgno 9210-003-209.pdf\n",
            "Processing file 1xE1hqX5HPfnXgtz-BszJt64bu7ghgo-Y 9210-003-210.pdf\n",
            "Processing file 1zS8rkqIxFwEfDT9HeD1NFeAbxzOYKAMX 9210-003-211.pdf\n",
            "Processing file 1suynU_UdXSs2-T3dxbdYElL6cGkfQmfg 9210-003-224.pdf\n",
            "Processing file 1I87WNVstNjBrljDz9El6oZ6TH55uDrC5 9210-003-601.pdf\n",
            "Processing file 114KdjaOU3Qbsy63lTIzgl9hhEQt_YMHx 9210-003-604.pdf\n",
            "Processing file 1zvXiTlJEW-cANkHPDKvWH9Eh6QVMwdeY 9210-003-606.pdf\n",
            "Processing file 1D0rHTrpG0TErw80eaK10r47y093SDAUL 9210-003-607.pdf\n",
            "Processing file 1RFQ09MgbsFOGVat599QhjYnxY9ANEacL 9210-003-611.pdf\n",
            "Processing file 1TUiFTusbbuJbiF5T92XIpIjq5s6AqSNC 9210-003-612.pdf\n",
            "Processing file 1vDeU8zXZcGzSxbQGHTSptGkTvIDlyRVC 9210-005-101.pdf\n",
            "Processing file 1E-VewfltA-TcqjlUcYZfvjTZiwKex6uZ 9210-005-102.pdf\n",
            "Processing file 1F1lH7FkCAMBJxhxeN3Qo9m52dMvV4f5u 9210-005-132.pdf\n",
            "Processing file 13c405LnqAm9bz4mUQ9fd7V3dPFaA30cN 9210-005-202.pdf\n",
            "Processing file 1T_Y8N3cyBXF1892En1_NDYfXSRBT6IFf 9210-005-205.pdf\n",
            "Processing file 1IIt90nmrhwMTau8KimX9CZWqt0hxtS2k 9210-005-206.pdf\n",
            "Processing file 1cjJIwcH26qRgpgVmfVIWldOxRM7c53uB 9210-005-208.pdf\n",
            "Processing file 1TzZqjRgnWQbNM_bETJ2Bcsl5An5zCs9Z 9210-005-209.pdf\n",
            "Processing file 1asArwphfXAWuRtbG0KOnVS6m7rUIAzUX 9210-005-210.pdf\n",
            "Processing file 1pTcFnyk_fJfoMX8ul8v764mDDzXgnhQ2 9210-005-211.pdf\n",
            "Processing file 1Z60O_CdtkOroRJGIQ_EQ_-LcpM2N1uw_ 9210-005-213.pdf\n",
            "Processing file 1_qcvNcFygur59YwMQx4nZ7tU5Nfet9zK 9210-005-214.pdf\n",
            "Processing file 1Jx7kFMMDfKXW1AULTLu_6hfTn8Vv1AUg 9210-008-101.pdf\n",
            "Processing file 1iibLQKaIyBoSgHDN3NBFK6V6OIf65icL 9210-008-203.pdf\n",
            "Processing file 1dGfuMVjgwFrdvPrp5Xe2HT4m3idziOnt 9210-008-204.pdf\n",
            "Processing file 1Y57i1EeLSLEXD_eQGNn_uvjrrvonh2Pr 9210-018-200.pdf\n",
            "Processing file 1Y5gcWNAb8j-J7AGY3NNVVbaK7PfD906q Filtros.txt\n",
            "Processing file 1UxaGkvtTj9pomdp7KR5Fs7lbUAxJT31e LSS.pdf\n",
            "Processing file 1R7LDR_9uN39o6FDUACiWSbmq9nEzXk5q Procedimiento Alta Patronal.txt\n",
            "Processing file 1dnoH0CJGEXUV8WULeXuG5HLgMOJTbJdT Reg_LSS_MACERF.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileURLRetrievalError",
          "evalue": "Failed to retrieve file url:\n\n\tCannot retrieve the public link of the file. You may need to change\n\tthe permission to 'Anyone with the link', or have had many accesses.\n\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1q1I1zt_ygtTYjYjKewFq4zf4g0NkxsVA\n\nbut Gdown can't. Please check connections and permissions.",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileURLRetrievalError\u001b[39m                     Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.12/site-packages/gdown/download.py:267\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     url = \u001b[43mget_url_from_gdrive_confirmation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.12/site-packages/gdown/download.py:55\u001b[39m, in \u001b[36mget_url_from_gdrive_confirmation\u001b[39m\u001b[34m(contents)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(\n\u001b[32m     56\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot retrieve the public link of the file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou may need to change the permission to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     58\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAnyone with the link\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, or have had many accesses. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     59\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     60\u001b[39m     )\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m url\n",
            "\u001b[31mFileURLRetrievalError\u001b[39m: Cannot retrieve the public link of the file. You may need to change the permission to 'Anyone with the link', or have had many accesses. Check FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mFileURLRetrievalError\u001b[39m                     Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(directorio_documentos):\n\u001b[32m      6\u001b[39m     os.makedirs(directorio_documentos)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mgdown\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mid_repositorio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdirectorio_documentos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cookies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m archivos_cargados = os.listdir(directorio_documentos)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m archivo \u001b[38;5;129;01min\u001b[39;00m archivos_cargados:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.12/site-packages/gdown/download_folder.py:325\u001b[39m, in \u001b[36mdownload_folder\u001b[39m\u001b[34m(url, id, output, quiet, proxy, speed, use_cookies, remaining_ok, verify, user_agent, skip_download, resume)\u001b[39m\n\u001b[32m    322\u001b[39m     files.append(local_path)\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m local_path = \u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://drive.google.com/uc?id=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cookies\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.12/site-packages/gdown/download.py:278\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[39m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    269\u001b[39m         message = (\n\u001b[32m    270\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFailed to retrieve file url:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    271\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou may still be able to access the file from the browser:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    276\u001b[39m             url_origin,\n\u001b[32m    277\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(message)\n\u001b[32m    280\u001b[39m filename_from_url = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    281\u001b[39m last_modified_time = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[31mFileURLRetrievalError\u001b[39m: Failed to retrieve file url:\n\n\tCannot retrieve the public link of the file. You may need to change\n\tthe permission to 'Anyone with the link', or have had many accesses.\n\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1q1I1zt_ygtTYjYjKewFq4zf4g0NkxsVA\n\nbut Gdown can't. Please check connections and permissions."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertir los archivos PDF descargados en un documento de texto"
      ],
      "metadata": {
        "id": "7XW2g8FsCakJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener archivos PDF en el directorio\n",
        "archivos_pdf = [f for f in os.listdir(directorio_documentos) if f.lower().endswith(\".pdf\")]\n",
        "\n",
        "for archivo in archivos_pdf:\n",
        "    ruta_archivo = os.path.join(directorio_documentos, archivo)\n",
        "    texto_extraido = \"\"\n",
        "    try:\n",
        "        with fitz.open(ruta_archivo) as documento:\n",
        "            for pagina in documento:\n",
        "                texto_extraido += pagina.get_text()\n",
        "\n",
        "                texto_extraido = texto_extraido.replace('\\xad', '')\n",
        "                texto_dividido = texto_extraido.splitlines()\n",
        "                texto_limpio = [re.sub(r'\\s+', ' ', re.sub(r'\\\\[a-zA-Z0-9]{1,6}', '', linea)).strip() for linea in texto_dividido]\n",
        "                lineas_utiles = [linea for linea in texto_limpio if linea.strip() and len(linea.strip().split()) > 1 and not linea.strip().isdigit()]\n",
        "\n",
        "                nombre_documento = os.path.splitext(archivo)[0] + \".txt\"\n",
        "                ruta_archivo_txt = os.path.join(directorio_documentos, nombre_documento)\n",
        "\n",
        "                with open(ruta_archivo_txt, \"w\", encoding=\"utf-8\") as archivo_txt:\n",
        "                  archivo_txt.write(\"\\n\".join(lineas_utiles))\n",
        "\n",
        "            print(f\"üìÑ Documento .txt creado: \\033[36m{ruta_archivo_txt}\\033[0m\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo procesar {archivo}: {e}\")\n",
        "\n",
        "print(f\"Total de documentos generados: \\033[32m{len(archivos_pdf)}\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLcARwERAMmn",
        "outputId": "fb94edaf-f3c1-406c-e0a1-98466efee31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-202.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-126.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-018-200.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-205.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-008-204.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-124.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-201.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-125.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-115.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-102.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/Reg_LSS_MACERF.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-008-203.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9000-002-002.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-209.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-213.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-607.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-210.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-101.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-114.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-132.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-200.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-211.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-208.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-206.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-612.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-606.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-101.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-604.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9000-002-003.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/LSS.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-008-101.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-611.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-112.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-202.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-211.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-005-214.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-210.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-224.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-209.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-601.txt\u001b[0m\n",
            "üìÑ Documento .txt creado: \u001b[36mdatos_RAG_LLM/9210-003-105.txt\u001b[0m\n",
            "Total de documentos generados: \u001b[32m41\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creaci√≥n de la base de datos FAISS"
      ],
      "metadata": {
        "id": "5yPOJfukCMoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para implementar el componente de recuperaci√≥n del sistema RAG, utilizamos FAISS (Facebook AI Similarity Search), una biblioteca dise√±ada por Meta para la b√∫squeda de similitudes entre vectores densos. Este sistema es utilizado en aplicaciones que requieren b√∫squedas r√°pidas y precisas en grandes vol√∫menes de datos, como recomendadores, motores de b√∫squeda sem√°ntica, y sistemas de respuesta a preguntas (Meta AI, 2017; Douze et al., 2024).\n",
        "\n",
        "En nuestro chatbot FAISS almacena los embeddings generados a partir de fragmentos del corpus documental, permitiendo recuperar los pasajes m√°s relevantes en tiempo real. A continuaci√≥n, describimos el flujo t√©cnico utilizado:\n",
        "\n",
        "1Ô∏è‚É£ Carga del Documento\n",
        "- `TextLoader` abre el archivo de texto en la ubicaci√≥n DATA_PATH.\n",
        "- `loader.load()` lee el contenido y lo almacena en documents.\n",
        "- Se imprime la cantidad de documentos cargados para verificaci√≥n.\n",
        "\n",
        "2Ô∏è‚É£ Divisi√≥n del Texto en Fragmentos (chunks)\n",
        "- `CharacterTextSplitter` divide los documentos en partes 1500 caracteres con un solapamiento de 500 caracteres.\n",
        "- Esto ayuda a mejorar las b√∫squedas sin perder el contexto entre fragmentos (LangChain, 2025)\n",
        "\n",
        "3Ô∏è‚É£ Generaci√≥n de Embeddings Sem√°nticos\n",
        "- `HuggingFaceEmbeddings` carga el modelo de embeddings, que convierte texto en vectores num√©ricos.\n",
        "- Estos vectores representan el significado de las palabras y permiten b√∫squedas sem√°nticas.\n",
        "\n",
        "4Ô∏è‚É£ Construcci√≥n del √≠ndice vectorial con FAISS\n",
        "- `FAISS.from_documents(docs, embeddings)` crea una base de datos donde cada fragmento de texto se convierte en un vector mediante los embeddings.\n",
        "- `FAISS` permite hacer b√∫squedas r√°pidas y eficientes dentro de este conjunto de datos vectoriales.\n",
        "\n",
        "\n",
        "**Referencias**\n",
        "\n",
        "* Meta AI. (2017, marzo 29). Faiss: A library for efficient similarity search. Engineering at Meta. Recuperado el 23 de junio de 2025 de https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/\n",
        "* Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazar√©, P.-E., et al. (2024). The Faiss library. arXiv. Recuperado el 23 de junio de https://arxiv.org/pdf/2401.08281\n",
        "* LangChain. (2025). Text Splitters. Recuperado el 22 de junio de 2025 de https://python.langchain.com/docs/concepts/text_splitters/"
      ],
      "metadata": {
        "id": "QZwlhy2Y3Sxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci√≥n para Crear la Base de Datos Vectorial (FAISS)\n",
        "def crea_vectores(documentos, embedding_model_name):\n",
        "\n",
        "  CHUNK_SIZE = 1500\n",
        "  CHUNK_OVERLAP = 500\n",
        "  documentos = []\n",
        "\n",
        "  if os.path.exists(directorio_documentos):\n",
        "\n",
        "    # Obtenemos la lista de los documentos\n",
        "    archivos = sorted([f for f in os.listdir(directorio_documentos) if f.endswith('.txt')])\n",
        "\n",
        "    if archivos:\n",
        "        for i, nombre_archivo in enumerate(archivos):\n",
        "            ruta_documentos = os.path.join(directorio_documentos, nombre_archivo)\n",
        "\n",
        "            print(f\"Cargando documento de: \\033[36m{ruta_documentos}\\033[0m\")\n",
        "            loader = TextLoader(ruta_documentos, encoding=\"utf-8\")\n",
        "            documento_cargado = loader.load()\n",
        "\n",
        "            documentos.extend(documento_cargado)\n",
        "\n",
        "            # Dividir el texto en chunks\n",
        "            # chunk_size y chunk_overlap son importantes para el rendimiento y la calidad\n",
        "\n",
        "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "            docs = text_splitter.split_documents(documentos)\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No se encontraron los documentos en el directorio.\")\n",
        "  else:\n",
        "    raise FileNotFoundError(f\"‚ùå El directorio '{directorio_documentos}' no fue encontrado.\")\n",
        "\n",
        "  print(f\"N√∫mero de documentos cargados: \\033[36m{len(documentos)}\\033[0m\")\n",
        "  print(f\"N√∫mero de chunks creados: \\033[36m{len(docs)}\\033[0m\")\n",
        "\n",
        "  # Inicializar el modelo de embeddings\n",
        "  print(f\"Cargando modelo de embeddings: \\033[36m{embedding_model_name}\\033[0m\")\n",
        "  # Langchain tiene un wrapper para HuggingFaceEmbeddings\n",
        "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "  # Crear la base de datos vectorial FAISS\n",
        "  print(\"Creando y poblando la base de datos vectorial FAISS\")\n",
        "  vector_store = FAISS.from_documents(docs, embeddings)\n",
        "  print(\"Base de datos vectorial \\033[32mcreada con √©xito\\033[0m.\")\n",
        "\n",
        "  return vector_store, embeddings"
      ],
      "metadata": {
        "id": "ZWKdWeWfakXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creaci√≥n del modelo LLM"
      ],
      "metadata": {
        "id": "PPOVwpSJCU1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos un modelo Large Language Model (LLM) optimizado para eficiencia y compatibilidad con entornos limitados de GPU. La estrategia se basa en la cuantizaci√≥n de 4‚ÄØbits, que reduce el consumo de memoria sin afectar en gran medida la calidad de las respuestas (Dettmers et al., 2023). Para ello implementamos los siguientes pasos:\n",
        "\n",
        "1Ô∏è‚É£Carga el modelo desde la biblioteca Hugging Face, permitiendo el uso de modelos preentrenados. Esto nos deja  aprovechar las capacidades ling√º√≠sticas ya desarrolladas sin necesidad de entrenar desde cero.\n",
        "\n",
        "2Ô∏è‚É£Optimiza el uso de memoria en GPU Mediante la integraci√≥n de BitsAndBytesConfig y la biblioteca bitsandbytes, configuramos load_in_4bit=True y bnb_4bit_quant_type=\"nf4\" para representar los pesos del modelo en solo 4‚ÄØbits. Esta reducci√≥n de precisi√≥n disminuye el uso de VRAM hasta en un 75‚ÄØ%, manteniendo un rendimiento casi id√©ntico (Hu et al., 2023; Hugging Face, 2024).\n",
        "\n",
        "3Ô∏è‚É£Detecta la disponibilidad de una GPU y carga el modelo en GPU si est√° disponible; de lo contrario, lo carga en CPU. Este paso garantiza escalabilidad y portabilidad en distintos entornos.\n",
        "\n",
        "4Ô∏è‚É£Configura el tokenizador para la generaci√≥n de texto, asegurando un procesamiento adecuado de las entradas en asegurando el encoding/decoding adecuado de texto en espa√±ol, incluyendo manejo de acentos y tokens especiales.\n",
        "\n",
        "**Referencias**\n",
        "\n",
        "* Dettmers, T., et al. (2023). Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA. Hugging Face Blog. Recuperado el 23 de junio de 2025 de https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
        "\n",
        "* Hugging Face. (2024). Quantization [Documentaci√≥n Transformers]. Recuperado el 23 de junio de 2025 de https://huggingface.co/docs/transformers/main_classes/quantization/"
      ],
      "metadata": {
        "id": "cIvY9jCztp6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def carga_llm(model_name):\n",
        "\n",
        "    # Configuraci√≥n de cuantizaci√≥n (para ahorrar VRAM en GPU)\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=False,\n",
        "    )\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU detectada. Cargando \\033[36m{model_name}\\033[0m\")\n",
        "\n",
        "        # Cargar tokenizador\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        tokenizer.pad_token = tokenizer.eos_token # Importante para generaci√≥n\n",
        "\n",
        "        # Cargar modelo con cuantizaci√≥n\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\" # Para distribuir el modelo autom√°ticamente en las GPUs disponibles\n",
        "        )\n",
        "        print(f\"Modelo \\033[36m{model_name}\\033[0m cargado en \\033[35mGPU\\033[0m.\")\n",
        "        return tokenizer, model\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        print(f\"Modelo \\033[36m{model_name}\\033[0m cargado en CPU.\")\n",
        "        return tokenizer, model"
      ],
      "metadata": {
        "id": "-xj8qO9fi9by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creaci√≥n de la base de datos vectorial y carga de LLM"
      ],
      "metadata": {
        "id": "CZevwwekCcmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta secci√≥n, primero construimos la base de recuperaci√≥n sem√°ntica indexando los documentos mediante embeddings y FAISS, lo que permite b√∫squedas r√°pidas y relevantes en grandes vol√∫menes documentales (Douze et al., 2024).\n",
        "\n",
        "Luego, cargamos un LLM preentrenado desde Hugging Face, cuantizado en 4 bits para optimizar el uso de memoria sin perder calidad del significado (Dettmers et al., 2023).\n",
        "\n",
        "**Referencias**\n",
        "* Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazar√©, P.-E., et al. (2024). The Faiss library. arXiv. Recuperado el 23 de junio de 2025 de https://doi.org/10.48550/arXiv.2401.08281\n",
        "* Dettmers, T., et al. (2023). Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA. Hugging Face Blog. Recuperado el 23 de junio de 2025 de https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
        "\n"
      ],
      "metadata": {
        "id": "ZxgeBN8qUc9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear la base de datos vectorial\n",
        "vector_store, embeddings_model = crea_vectores(directorio_documentos, EMBEDDING_MODEL_NAME)\n",
        "# Cargar el LLM\n",
        "tokenizer, llm_model = carga_llm(LLM_MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925,
          "referenced_widgets": [
            "6108ce83ad1b4249821c901665018d27",
            "4dfb48f714264cb297fc702ecf4b5ecd",
            "c811e59d30ef4988b1e432e72389fab0",
            "875ba73f53384554a3dfe59d820258c8",
            "486048964eda45d781110d36b9fe339b",
            "7cce340849e84e968bcea6646a989f6a",
            "c31d15c6a562403696d2c7bdefa7a825",
            "c84e73d908fd4f7e88cd69d5e353d173",
            "aed1c3ff8c29481080b5fcf2123e474f",
            "1f444f7be910437f9f565194021f58ec",
            "372ca5ef171947c9944dd6f7b525d7e9"
          ]
        },
        "id": "croT_E256HcQ",
        "outputId": "c5c1a855-dbe8-4d47-dbd1-331a54942922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9000-002-002.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9000-002-003.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-101.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-105.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-112.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-114.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-115.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-124.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-125.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-126.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-200.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-201.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-202.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-209.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-210.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-211.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-224.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-601.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-604.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-606.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-607.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-611.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-003-612.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-101.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-102.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-132.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-202.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-205.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-206.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-208.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-209.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-210.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-211.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-213.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-005-214.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-008-101.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-008-203.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-008-204.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/9210-018-200.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/Filtros.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/LSS.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/Procedimiento Alta Patronal.txt\u001b[0m\n",
            "Cargando documento de: \u001b[36mdatos_RAG_LLM/Reg_LSS_MACERF.txt\u001b[0m\n",
            "N√∫mero de documentos cargados: \u001b[36m43\u001b[0m\n",
            "N√∫mero de chunks creados: \u001b[36m2260\u001b[0m\n",
            "Cargando modelo de embeddings: \u001b[36msentence-transformers/all-MiniLM-L6-v2\u001b[0m\n",
            "Creando y poblando la base de datos vectorial FAISS\n",
            "Base de datos vectorial \u001b[32mcreada con √©xito\u001b[0m.\n",
            "GPU detectada. Cargando \u001b[36mmistralai/Mistral-7B-Instruct-v0.2\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6108ce83ad1b4249821c901665018d27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo \u001b[36mmistralai/Mistral-7B-Instruct-v0.2\u001b[0m cargado en \u001b[35mGPU\u001b[0m.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creaci√≥n del pipeline"
      ],
      "metadata": {
        "id": "mCq03MIwCguq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuramos el pipeline de generaci√≥n de texto basado modelo LLM preentrenado (LLM) utilizando la clase pipeline de Hugging Face Transformers. Este pipeline se encarga de recibir las consultas del usuario, procesarlas mediante el tokenizador y generar una respuesta coherente, fundamentada en la informaci√≥n recuperada por el sistema RAG. (HuggingFace, 2024)\n",
        "\n",
        "Esta etapa es esencial en el funcionamiento del chatbot, ya que define c√≥mo el modelo genera las respuestas, incluyendo par√°metros que afectan su longitud, creatividad y precisi√≥n sem√°ntica. A continuaci√≥n definimos estos par√°metros:\n",
        "\n",
        "‚öôÔ∏è Par√°metros principales\n",
        "- `model=llm_model`: especifica el modelo de lenguaje preentrenado que se utilizar√°.\n",
        "- `tokenizer=tokenizer`: define el tokenizador asociado al modelo, responsable de convertir texto en tokens y viceversa.\n",
        "- `max_new_tokens=800`: establece la longitud m√°xima de texto generado, en n√∫mero de tokens.\n",
        "- `do_sample=True`: habilita el muestreo aleatorio en la generaci√≥n de texto, en lugar de elegir siempre la opci√≥n m√°s probable.\n",
        "- `temperature=0.8`: ajusta el nivel de creatividad; valores m√°s altos generan respuestas m√°s variadas.\n",
        "- `top_k=50`: limita la selecci√≥n a los 50 tokens m√°s probables antes de elegir aleatoriamente.\n",
        "- `top_p=0.95`: aplica ‚Äúnucleus sampling‚Äù, seleccionando entre los tokens con una probabilidad acumulada del 95 %.\n",
        "- `eos_token_id=tokenizer.eos_token_id`: define el token de fin de secuencia para detener la generaci√≥n cuando se alcanza.\n",
        "- `return_full_text=False`: indica que se debe devolver solo el texto generado, excluyendo el texto de entrada.\n",
        "\n",
        "**Referencias**\n",
        "* Hugging Face. (2024). Text Generation Pipelines. Recuperado el 23 de junio de 2025 de https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.TextGenerationPipeline"
      ],
      "metadata": {
        "id": "UFw6eLaZ0jt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el pipeline de HuggingFace para el LLM\n",
        "# Se especifica temperature para controlar la creatividad y max_new_tokens para limitar la longitud de la respuesta\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llm_model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=1500, # Longitud m√°xima de la respuesta generada\n",
        "    do_sample=True,\n",
        "    temperature=0.8, # Creatividad del modelo (0.0 a 1.0, m√°s alto es m√°s creativo)\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    eos_token_id=tokenizer.eos_token_id, # Asegura que el modelo se detenga al generar el token EOS\n",
        "    return_full_text = False\n",
        ")"
      ],
      "metadata": {
        "id": "SxG5PU0etiHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b81e0309-e2d5-475f-9a43-a8dfd28bc88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Langchain para responder preguntas de forma contextualizada"
      ],
      "metadata": {
        "id": "U8Z7HWOiCnyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este framework permite ejecutar f√°cilmente el flujo entre los componentes del sistema: recuperaci√≥n, procesamiento y generaci√≥n de texto (Zhou et al., 2023). En este pipeline, la consulta del usuario activa los siguientes pasos:\n",
        "\n",
        "1Ô∏è‚É£ **Integraci√≥n del modelo de lenguaje (LLM)**\n",
        "\n",
        "El modelo de lenguaje preentrenado, cargado desde Hugging Face y previamente cuantizado para eficiencia, se conecta mediante HuggingFacePipeline. Esto permite usar el mismo modelo con par√°metros de control definidos en la etapa anterior (como temperatura, top_k, etc.).\n",
        "\n",
        "2Ô∏è‚É£ **Recuperador sem√°ntico (Retriever)**\n",
        "\n",
        "Se crea un retriever a partir del √≠ndice vectorial FAISS, que permite recuperar los fragmentos de texto m√°s relevantes para una consulta dada. Este componente se encarga de realizar la b√∫squeda sem√°ntica dentro del corpus documental del IMSS, usando comparaciones vectoriales.\n",
        "\n",
        "3Ô∏è‚É£ **Prompt personalizado**\n",
        "\n",
        "Se define un prompt instructivo que gu√≠a al modelo para:\n",
        "* Ser claro y amigable.\n",
        "* Evitar generar respuestas sin fundamento (‚Äúalucinaciones‚Äù).\n",
        "* Basarse √∫nicamente en los documentos proporcionados.\n",
        "* El prompt representa una capa cr√≠tica de control, alineando el comportamiento del LLM con la √©tica y objetivos del chatbot (Zhou et al., 2023).\n",
        "\n",
        "4Ô∏è‚É£ **Construcci√≥n de la cadena RAG**\n",
        "Se construye una cadena de tipo RetrievalQA en LangChain, que:\n",
        "\n",
        "* Toma como entrada la pregunta del usuario.\n",
        "* Recupera fragmentos relevantes del corpus mediante FAISS.\n",
        "* Genera una respuesta a partir de los documentos recuperados.\n",
        "* Devuelve una salida textual clara, fundamentada y √∫til.\n",
        "\n",
        "Este flujo cierra el ciclo RAG: recuperaci√≥n de contexto + generaci√≥n controlada, optimizando la calidad de las respuestas sin necesidad de entrenamiento adicional del modelo.\n",
        "\n",
        "**Referencias**\n",
        "* Zhou, M., et al. (2023). DocPrompting: Generating documents with retrieval-augmented language models. Proceedings of the 2023 Conference of the Association for Computational Linguistics. https://arxiv.org/abs/2305.06983\n",
        "\n"
      ],
      "metadata": {
        "id": "WUphf0TDWpK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente c√≥digo, implementa un flujo completo basado en RAG (Retrieval-Augmented Generation) utilizando LangChain para responder preguntas de forma contextualizada.\n",
        "- Configura un modelo de lenguaje (LLM) a trav√©s de Hugging Face mediante `HuggingFacePipeline`.\n",
        "- Crea un retriever que obtiene fragmentos de informaci√≥n relevantes desde una base de datos vectorial.\n",
        "- Define un prompt personalizado que orienta al modelo para responder con amabilidad, precisi√≥n y sin generar informaci√≥n inventada.\n",
        "- Construye una cadena RAG que:\n",
        "- Recupera informaci√≥n contextual.\n",
        "- Genera una respuesta basada en dicha informaci√≥n.\n",
        "- Devuelve una salida textual clara y √∫til."
      ],
      "metadata": {
        "id": "WmdPl-X92iM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "\n",
        "# Crear el retriever\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 6}) # Recupera los 6 chunks m√°s relevantes\n",
        "\n",
        "# --- Definir el Prompt Template ---\n",
        "# Este prompt gu√≠a al LLM sobre c√≥mo debe responder\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "    Eres un asistente de IA amigable y eficiente que responde preguntas relacionadas con procedimientos de afiliaci√≥n.\n",
        "    Debes responder √∫nicamente con base en el contexto proporcionado, sin mencionar los documentos ni fuentes consultadas.\n",
        "    Si la informaci√≥n del contexto no es suficiente para responder con certeza, indica amablemente que no est√°s seguro.\n",
        "    Cada respuesta debe comenzar con una frase introductoria distinta que transmita amabilidad, inter√©s y cercan√≠a (por ejemplo: \"Con gusto te explico...\" o \"Qu√© buena pregunta, aqu√≠ tienes la informaci√≥n...\"), y procura variar esa frase en cada ocasi√≥n.\n",
        "    Al final de cada respuesta, incluye:\n",
        "    ‚Äì Una frase que invite al usuario a contactar al √°rea normativa en caso de dudas adicionales (por ejemplo: \"Si tienes alguna otra duda, no dudes en ponerte en contacto con el √°rea normativa correspondiente.\")\n",
        "    ‚Äì Una advertencia clara indicando que el contenido fue generado por inteligencia artificial, que puede contener errores, y que es importante verificar la informaci√≥n antes de tomar decisiones.\n",
        "\n",
        "\n",
        "    Contexto: {context}\n",
        "\n",
        "    Pregunta: {question}\n",
        "\n",
        "    Respuesta √∫til:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# --- Construir la cadena de RAG con Langchain Expression Language (LCEL) ---\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt_template\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "DL508LcWr_gE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b71ad590-1bac-498f-af70-4121f12b3f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_6292/1381692076.py:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=llm_pipeline)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot"
      ],
      "metadata": {
        "id": "mIWmsYSwCxri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funci√≥n `chat_with_rag` recibe una pregunta del usuario y devuelve una respuesta generada a partir de la cadena RAG, la cual combina recuperaci√≥n de informaci√≥n y generaci√≥n de texto.\n",
        "\n",
        "Se crea una aplicaci√≥n web interactiva que permite:\n",
        "- Ingresar preguntas en un campo de texto.\n",
        "- Obtener respuestas generadas por IA basadas en informaci√≥n recuperada.\n",
        "- Visualizar un t√≠tulo, una descripci√≥n y ejemplos que facilitan la interacci√≥n del usuario.\n",
        "\n",
        "La creaci√≥n de la interfaz incluye las siguientes configuraciones clave:\n",
        "- `fn=chat_with_rag`: conecta la funci√≥n principal del asistente con la interfaz.\n",
        "- `inputs`: define el campo de entrada donde el usuario formula su pregunta.\n",
        "- `outputs`: especifica el √°rea donde se muestra la respuesta generada.\n",
        "- `title`: establece el t√≠tulo visible de la aplicaci√≥n.\n",
        "- `description`: proporciona una breve explicaci√≥n sobre el prop√≥sito de la interfaz.\n",
        "- `examples`: muestra preguntas sugeridas que el usuario puede seleccionar para probar el sistema.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uYbcvV_N3yb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Funci√≥n para la interfaz de Gradio ---\n",
        "def chat_with_rag(user_query):\n",
        "    try:\n",
        "        response = rag_chain.invoke(user_query)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"Ocurri√≥ un error: {e}\"\n",
        "\n",
        "# --- Interfaz de Gradio ---\n",
        "print(\"Creando interfaz de Gradio...\")\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=chat_with_rag,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Escribe tu pregunta aqu√≠...\", label=\"Tu Pregunta\"),\n",
        "    outputs=gr.Textbox(label=\"Respuesta del Asistente\"),\n",
        "    title=\"Consultor Virtual de la Unidad de Incorporaci√≥n al Seguro Social ‚Äì Potenciado por IA\",\n",
        "    description=\"Aclaramos tus consultas sobre los procedimientos de afiliaci√≥n.\",\n",
        "    examples=[\"Expl√≠came el procedimiento de alta patronal\", \"¬øQu√© modalidades validas existen?\", \"¬øC√≥mo doy de alta un patron persona f√≠sica?\"]\n",
        ")\n",
        "\n",
        "# Lanzar la interfaz\n",
        "iface.launch(share=True) # share=True para generar un enlace p√∫blico temporal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "t7r5Rf4wsMPC",
        "outputId": "35c314fc-b0ae-4d6a-82c9-e958dab2656b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creando interfaz de Gradio...\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://62e9020ce8a6345c57.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://62e9020ce8a6345c57.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusiones:**"
      ],
      "metadata": {
        "id": "Kx-dZSFJz9cK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### **Incluyan sus conclusiones de la actividad chatbot LLM + RAG:**\n",
        "\n",
        "\n",
        "\n",
        "None"
      ],
      "metadata": {
        "id": "3w3usdaC0BCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fin de la actividad chatbot: LLM + RAG**"
      ],
      "metadata": {
        "id": "CtB5Q3m41YQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revisar si queremos formato markdown"
      ],
      "metadata": {
        "id": "63OXBodwLGyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://faiss.ai/"
      ],
      "metadata": {
        "id": "WdbER9uis7c7"
      }
    }
  ]
}